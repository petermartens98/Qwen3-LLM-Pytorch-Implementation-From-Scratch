{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "QIwBDrKlDRjI"
      ],
      "authorship_tag": "ABX9TyPDZ8PDUYBHumxXLjeajFBK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Qwen 3 LLM Implementation From Scratch***"
      ],
      "metadata": {
        "id": "QIwBDrKlDRjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Overview**\n",
        "- Lightweight LLM inspired by Qwen3, built from scratch in PyTorch.\n",
        "- Implements modern transformer components including RMSNorm, Rotary Position Embeddings (RoPE), Grouped-Query Attention (GQA), and SwiGLU feed-forward layers.\n",
        "- Trained using a hybrid Muon + AdamW optimizer setup with causal masking, efficient batching, and evaluation utilities.\n",
        "- Includes full training pipeline, model loading, and interactive text generation demos for hands-on experimentation."
      ],
      "metadata": {
        "id": "GRelbbZb4CnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Useful Materials**\n",
        "- Qwen 3 Technical Report PDF: https://arxiv.org/pdf/2505.09388\n",
        "- Qwen 3 GitHub Repo: https://github.com/QwenLM/Qwen3"
      ],
      "metadata": {
        "id": "UYUcr7AiDRl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step by Step Overview (Table of Contents)**\n",
        "1. Imports\n",
        "2. Utility Functions (set_seed, ...)\n",
        "3. Model Configuration\n",
        "4. Key/Value Head Expansion Function\n",
        "5. Muron Optimizer (Orthogonalized Momentum via Newton–Schulz)\n",
        "6. Data Loading and Caching\n",
        "7. TextTokenDataset Class\n",
        "8. Rotary Position Embeddings (RoPE)\n",
        "9. Grouped-Query Attention (GQA)\n",
        "10. SwiGLU Feed-Forward Network (FFN)\n",
        "11. Transformer block (attention + FFN + RMSNorm + residuals)\n",
        "12. Language model class (MinimalLLM)\n",
        "13. Evaluation function (loss, accuracy, perplexity)\n",
        "14. Optimizer setup (hybrid Muon + AdamW)\n",
        "15. Training loop (AMP, grad accumulation, schedulers)\n",
        "16. Training Script\n",
        "17. Model Loading\n",
        "18. Model Inference - Autoregressive Text Generation and Chat Interactive Inference"
      ],
      "metadata": {
        "id": "YY_WVwe1DTDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Imports**"
      ],
      "metadata": {
        "id": "t_s7dsjwEiWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os                                          # File system operations (creating folders, path checking, etc.)\n",
        "import time                                        # Timing utilities, measuring time\n",
        "import math                                        # Standard math operations (e.g. sqrt, exp, cos)\n",
        "import random                                      # Python's random number utilities (used for seeding)\n",
        "import pickle                                      # Python object serialization (used to save/load preprocessed datasets)\n",
        "import warnings                                    # Suppress or handle warnings\n",
        "\n",
        "import numpy as np                                 # Numerical computing library, used for random seeding and general array ops\n",
        "import torch                                       # PyTorch main library\n",
        "import torch.nn as nn                              # Neural network modules like Linear, Embedding, etc.\n",
        "import torch.nn.functional as F                    # Functional interface for operations like cross_entropy, silu, etc.\n",
        "from torch.utils.data import Dataset, DataLoader  # Base class and utilities for loading datasets\n",
        "from torch.cuda.amp import autocast, GradScaler    # Automatic Mixed Precision (AMP) tools for faster/lower-memory training\n",
        "\n",
        "from datasets import load_dataset                  # Hugging Face Datasets library for streaming large datasets\n",
        "from tqdm import tqdm                              # Progress bar visualization library, great for loops\n",
        "from transformers import AutoTokenizer             # Load pretrained tokenizers from HuggingFace with one line\n",
        "\n",
        "from dataclasses import dataclass                  # Define simple classes for configs with less boilerplate\n",
        "from typing import List, Optional                  # Type hints for better readability and tooling\n",
        "\n",
        "warnings.filterwarnings('ignore')                  # Silences warnings for cleaner outputs during training\n"
      ],
      "metadata": {
        "id": "JuYP2wnaDcXV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Utility Functions**"
      ],
      "metadata": {
        "id": "eF3d1agbFiOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Seed Utility Function Ensuring Reproducibiity\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Set all seeds to {seed}\")"
      ],
      "metadata": {
        "id": "5iwR1KRNFlUO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Model Configuration**"
      ],
      "metadata": {
        "id": "L28POJfmIDOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"\n",
        "    Compact Qwen3-style configuration for reasoning-focused experiments.\n",
        "    --------------------------------------------------------------------\n",
        "    This configuration mirrors Qwen3’s architectural principles (GQA, RMSNorm,\n",
        "    no attention bias, SwiGLU FFN) but at a much smaller scale for efficient\n",
        "    prototyping and reasoning research on limited hardware.\n",
        "    \"\"\"\n",
        "\n",
        "    # ----------------------\n",
        "    # Model architecture\n",
        "    # ----------------------\n",
        "    d_model: int = 384                    # Hidden dimension size for each token embedding (Qwen3 uses 2K–8K)\n",
        "    n_heads: int = 8                      # Total number of attention heads for multi-head attention\n",
        "    n_layers: int = 6                     # Number of Transformer layers (Qwen3 ranges 28–94)\n",
        "    d_ff: int = 1536                      # Feedforward dimension (≈4 × d_model, using SwiGLU in Qwen3)\n",
        "    batch_size: int = 24                  # Mini-batch size for training\n",
        "    max_steps: int = 2000                 # Total number of training iterations (smaller-scale training)\n",
        "\n",
        "    # ----------------------\n",
        "    # Qwen3-specific architectural tweaks\n",
        "    # ----------------------\n",
        "    n_kv_heads: int = 4                   # Number of KV heads for Grouped Query Attention (GQA)\n",
        "    sliding_window: int = 4096            # Context chunk size; large default effectively disables sliding\n",
        "    attention_bias: bool = False          # Qwen3 removes QKV bias for training stability\n",
        "    rms_norm_eps: float = 1e-6            # Stabilizing epsilon for RMSNorm (Qwen3 uses 1e-6)\n",
        "\n",
        "    # ----------------------\n",
        "    # Training hyperparameters\n",
        "    # ----------------------\n",
        "    gradient_accumulation_steps: int = 4  # Accumulate gradients to simulate larger batch size\n",
        "    muon_lr: float = 0.01                 # Learning rate (placeholder; scaled for small models)\n",
        "\n",
        "    # ----------------------\n",
        "    # Data handling parameters\n",
        "    # ----------------------\n",
        "    max_seq_len: int = 512                # Max sequence length per training sample\n",
        "    num_documents: int = 2000             # Dataset size (for testing or research)\n",
        "    max_tokens: int = 500000              # Limit total processed tokens to manage memory\n",
        "\n",
        "    # ----------------------\n",
        "    # Evaluation\n",
        "    # ----------------------\n",
        "    eval_every: int = 500                 # Frequency of evaluation during training\n",
        "    eval_steps: int = 100                 # Number of evaluation steps\n",
        "\n",
        "    # ----------------------\n",
        "    # Regularization & stability\n",
        "    # ----------------------\n",
        "    weight_decay: float = 0.1             # L2 regularization strength\n",
        "    dropout: float = 0.1                  # Dropout rate; Qwen3 dense models often disable dropout\n",
        "    grad_clip: float = 1.0                # Gradient clipping threshold\n",
        "\n",
        "    # ----------------------\n",
        "    # Technical settings\n",
        "    # ----------------------\n",
        "    use_amp: bool = True                  # Enable automatic mixed precision for faster, lower-memory training\n",
        "    vocab_size: Optional[int] = None      # Vocabulary size (Qwen3 uses 151,669 BBPE tokens)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Per-head dimension for attention projections\n",
        "        self.d_k = self.d_model // self.n_heads\n",
        "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        # Grouped Query Attention (GQA) consistency check\n",
        "        assert self.n_heads % self.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
        "\n",
        "        # Each Key/Value head is shared by multiple Query heads\n",
        "        # Example: n_heads=8, n_kv_heads=4 → each KV head serves 2 Q heads\n",
        "        self.n_kv_groups = self.n_heads // self.n_kv_heads"
      ],
      "metadata": {
        "id": "qu826vEgHpED"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Key/Value Head Expansion for Grouped-Query Attention**\n",
        "- In GQA, multiple query heads share a smaller number of key/value heads to save memory and compute.\n",
        "- This helper function \"expands\" the K/V tensors so that each query head has a corresponding K/V pair to attend to.\n",
        "## **Example:**\n",
        "- if n_heads = 8 and n_kv_heads = 4 → each K/V head is repeated twice.\n",
        "- Input shape:  (batch, n_kv_heads, seq_len, head_dim)\n",
        "- Output shape: (batch, n_heads,    seq_len, head_dim)\n",
        "- This effectively enables multi-head queries to reuse fewer key/value projections efficiently.\n",
        "\n"
      ],
      "metadata": {
        "id": "ULI_CVdpOLOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## **4. Key/Value Head Expansion for Multi-Query or Grouped-Query Attention**\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Expands the number of key/value heads to match the number of attention heads\n",
        "    in multi-query or grouped-query attention setups.\n",
        "\n",
        "    Args:\n",
        "        hidden_states: Tensor of shape (batch, num_kv_heads, seq_len, head_dim)\n",
        "        n_rep: Number of repetitions for each key/value head\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape (batch, num_kv_heads * n_rep, seq_len, head_dim)\n",
        "\n",
        "    Example:\n",
        "        Suppose hidden_states has shape (2, 2, 4, 8) — 2 batches, 2 KV heads,\n",
        "        sequence length 4, and head dimension 8.\n",
        "\n",
        "        If n_rep = 4 (e.g., 8 attention heads / 2 KV heads),\n",
        "        the output will have shape (2, 8, 4, 8):\n",
        "        - Each KV head is repeated 4 times to match total attention heads.\n",
        "    \"\"\"\n",
        "    batch, num_kv_heads, slen, head_dim = hidden_states.shape\n",
        "\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "\n",
        "    # Insert new dimension → expand → flatten\n",
        "    # (b, kv, s, d) → (b, kv, 1, s, d) → (b, kv, n_rep, s, d) → (b, kv*n_rep, s, d)\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_kv_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_kv_heads * n_rep, slen, head_dim)\n"
      ],
      "metadata": {
        "id": "uQDlJqhCOLWE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Muron Optimizer (Orthogonalized Momentum via Newton–Schulz)**\n",
        "\n",
        "- The Muon optimizer is a stability-oriented optimizer that applies Newton–Schulz orthogonalization to the gradient or momentum updates.\n",
        "\n",
        "- Instead of directly applying raw gradient steps (which can accumulate correlated noise or directional bias), Muon reprojects updates onto an approximately orthogonal basis.\n",
        "    - This helps preserve feature diversity and prevent parameter collapse—especially important in large, low-rank or transformer-style models like Qwen.\n",
        "\n",
        "### **orthogonalize_newton_schulz Function**\n",
        "- uses a polynomially approximated Newton–Schulz iteration to find the “zeroth power” of GᵀG (i.e., G * (GᵀG)⁻¹/²).\n",
        "  - This approximates orthogonalization efficiently without an expensive matrix inverse or SVD.\n",
        "\n",
        "### **Example Flow:**\n",
        "\n",
        "#### ***Step 1: Start with a gradient tensor***\n",
        "\n",
        "$$\n",
        "G =\n",
        "\\begin{bmatrix}\n",
        "1.0 & 2.0 \\\\\n",
        "3.0 & 4.0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "- This gradient is *not orthogonal* — its columns are correlated.\n",
        "\n",
        "---\n",
        "\n",
        "#### ***Step 2: Normalize for numerical stability***\n",
        "$$\n",
        "G_{\\text{norm}} = \\frac{G}{\\|G\\|_F}\n",
        "$$\n",
        "\n",
        "\n",
        "- This scales \\( G \\) to unit Frobenius norm, preventing exploding values during iteration.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 3: Newton–Schulz iteration\n",
        "- Each Newton–Schulz iteration refines \\( X \\) toward orthogonality via the polynomial update:\n",
        "\n",
        "$$\n",
        "X_{t+1} = \\alpha X_t + \\big( \\beta (X_t X_t^\\top) + \\gamma (X_t X_t^\\top X_t X_t^\\top) \\big) X_t\n",
        "$$\n",
        "\n",
        "- The coefficients $\\alpha = 3.4445$, $\\beta = -4.7750$, and $\\gamma = 2.0315$ are carefully tuned to balance convergence speed and numerical stability, ensuring that the Newton–Schulz iteration  \n",
        "\n",
        "---\n",
        "\n",
        "#### ***Step 4: Result — Orthogonalized Gradient***\n",
        "- After 5 iterations, \\( X \\) approaches:\n",
        "$$\n",
        "X_{orth} =\n",
        "\\begin{bmatrix}\n",
        "-0.89 & 0.46 \\\\\n",
        "0.46 & 0.89\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "- Now the columns are approximately **orthogonal** and **unit-norm**.\n",
        "\n",
        "---\n",
        "\n",
        "#### ***Step 5: Parameter Update***\n",
        "- Muon applies momentum, Nesterov correction (if enabled), and updates weights:\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\cdot X_{orth}\n",
        "$$\n",
        "- This ensures stable and diverse gradient directions during training.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2KAPrxjddEL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.compile\n",
        "def orthogonalize_newton_schulz(G: torch.Tensor, num_iters: int = 5) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Orthogonalizes a tensor using the Newton–Schulz iterative method.\n",
        "\n",
        "    This function projects the input matrix G toward an orthogonal basis\n",
        "    (approximate zeroth power of GᵀG) using a numerically stable polynomial\n",
        "    approximation.\n",
        "\n",
        "    Args:\n",
        "        G (torch.Tensor): Input tensor (matrix or batch of matrices).\n",
        "        num_iters (int): Number of Newton–Schulz iterations (default: 5).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Orthogonalized tensor with approximately unit-norm columns.\n",
        "    \"\"\"\n",
        "    assert G.ndim >= 2, \"Input must have at least 2 dimensions.\"\n",
        "\n",
        "    # Coefficients for the polynomial update (empirically chosen)\n",
        "    alpha, beta, gamma = 3.4445, -4.7750, 2.0315\n",
        "\n",
        "    # Convert to bfloat16 for stability and performance\n",
        "    X = G.bfloat16()\n",
        "\n",
        "    # Ensure the \"tall\" matrix case is handled consistently\n",
        "    transpose_needed = G.size(-2) > G.size(-1)\n",
        "    if transpose_needed:\n",
        "        X = X.mT\n",
        "\n",
        "    # Normalize to unit Frobenius norm (stabilizes iteration)\n",
        "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
        "\n",
        "    # Newton–Schulz iterations to approach orthogonality\n",
        "    for _ in range(num_iters):\n",
        "        XXt = X @ X.mT\n",
        "        poly_term = beta * XXt + gamma * (XXt @ XXt)\n",
        "        X = alpha * X + poly_term @ X\n",
        "\n",
        "    if transpose_needed:\n",
        "        X = X.mT\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "class Muon(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Muon Optimizer: Momentum Orthogonalized via Newton–Schulz\n",
        "\n",
        "    This optimizer extends classical momentum/Nesterov updates by\n",
        "    orthogonalizing the gradient using the Newton–Schulz iteration.\n",
        "    It helps stabilize updates in overparameterized regimes and can\n",
        "    improve convergence for large, structured models.\n",
        "\n",
        "    Key points:\n",
        "    - Momentum: Maintains an exponential moving average of past gradients\n",
        "      to smooth updates and accelerate convergence.\n",
        "    - Nesterov Momentum: Looks ahead by applying the gradient correction\n",
        "      based on the estimated future position, improving stability\n",
        "      and convergence speed.\n",
        "\n",
        "    Args:\n",
        "        params (iterable): Model parameters to optimize.\n",
        "        lr (float): Learning rate (default: 0.02).\n",
        "        momentum (float): Momentum coefficient (default: 0.95).\n",
        "        nesterov (bool): Whether to apply Nesterov momentum (default: True).\n",
        "        ns_steps (int): Number of Newton–Schulz iterations for orthogonalization (default: 5).\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
        "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr = group[\"lr\"]\n",
        "            momentum = group[\"momentum\"]\n",
        "            use_nesterov = group[\"nesterov\"]\n",
        "            ns_steps = group[\"ns_steps\"]\n",
        "\n",
        "            for param in group[\"params\"]:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = param.grad\n",
        "                state = self.state[param]\n",
        "\n",
        "                # Initialize momentum buffer on first update\n",
        "                if \"momentum_buffer\" not in state:\n",
        "                    state[\"momentum_buffer\"] = torch.zeros_like(grad)\n",
        "\n",
        "                momentum_buf = state[\"momentum_buffer\"]\n",
        "\n",
        "                # Exponential moving average update of gradient\n",
        "                momentum_buf.lerp_(grad, 1 - momentum)\n",
        "\n",
        "                # Apply Nesterov correction if enabled\n",
        "                if use_nesterov:\n",
        "                    grad = grad.lerp(momentum_buf, momentum)\n",
        "                else:\n",
        "                    grad = momentum_buf\n",
        "\n",
        "                # Orthogonalize the gradient update\n",
        "                grad = orthogonalize_newton_schulz(grad, num_iters=ns_steps)\n",
        "\n",
        "                # Adaptive learning rate scaling based on matrix shape\n",
        "                aspect_ratio = max(1.0, param.size(-2) / param.size(-1))\n",
        "                scale = (aspect_ratio ** 0.5)\n",
        "\n",
        "                # Parameter update\n",
        "                param.add_(grad.view_as(param), alpha=-lr * scale)\n"
      ],
      "metadata": {
        "id": "7sT154GgWjB8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Data Loading & Caching**\n",
        "\n",
        "Loading and processing large text corpora can be time-consuming. This step implements an efficient **data loading and caching pipeline** to minimize redundant work and speed up repeated experiments.\n",
        "\n",
        "#### **Key features of this pipeline include:**\n",
        "\n",
        "* **Caching mechanism**: Saves tokenized data to disk and reloads it if available, avoiding repeated processing.\n",
        "* **Configurable dataset size**: Limits the number of documents processed using `config.num_documents`.\n",
        "* **Token limit enforcement**: Ensures the total number of tokens does not exceed `config.max_tokens`.\n",
        "* **Pretrained tokenizer**: Uses HuggingFace’s SmolLM tokenizer and automatically handles missing `pad_token`.\n",
        "* **Streaming dataset loading**: Efficiently loads the SmolLM corpus without overwhelming memory.\n",
        "* **Truncation of long documents**: Limits document length (e.g., first 3000 characters) to prevent excessively long inputs.\n",
        "* **Progress tracking**: Displays tokenization progress using `tqdm`.\n",
        "* **Automatic vocab size update**: Sets `config.vocab_size` based on the tokenizer.\n",
        "* **Reproducibility & consistency**: Guarantees consistent processed data across runs via caching.\n",
        "\n",
        "This ensures that training and experimentation are faster, memory-efficient, and reproducible.\n"
      ],
      "metadata": {
        "id": "M9ZghmT0biVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_cache_data(config: ModelConfig, cache_dir: str = \"data_cache\"):\n",
        "    \"\"\"\n",
        "    Load and cache tokenized data for model training to avoid redundant preprocessing.\n",
        "\n",
        "    This function ensures efficient data handling by:\n",
        "    1. Checking for pre-existing cached data to skip reprocessing.\n",
        "    2. Loading a tokenizer and assigning a pad token if missing.\n",
        "    3. Streaming a dataset to limit memory usage and control document size.\n",
        "    4. Tokenizing each document into integer token IDs compatible with the model.\n",
        "    5. Truncating the total number of tokens to `config.max_tokens` to fit memory limits.\n",
        "    6. Saving the processed data to a cache file for faster subsequent runs.\n",
        "\n",
        "    Args:\n",
        "        config (ModelConfig): Configuration object with dataset and token limits.\n",
        "        cache_dir (str): Directory where cached tokenized data will be stored.\n",
        "\n",
        "    Returns:\n",
        "        texts (List[str]): List of raw text documents.\n",
        "        tokenizer (PreTrainedTokenizer): HuggingFace tokenizer used to encode text.\n",
        "        tokens (List[int]): Flattened list of token IDs ready for model input.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the cache directory exists; create if it doesn't\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    # Construct a cache file path that encodes the dataset size and token limit\n",
        "    cache_file = f\"{cache_dir}/tokenized_data_{config.num_documents}_{config.max_tokens}.pkl\"\n",
        "\n",
        "    # ----------------------------\n",
        "    # 1. Attempt to load cached data\n",
        "    # ----------------------------\n",
        "    if os.path.exists(cache_file):\n",
        "        print(f\"Loading cached data from {cache_file}\")\n",
        "        with open(cache_file, 'rb') as f:\n",
        "            cached_data = pickle.load(f)\n",
        "\n",
        "        # Extract cached components\n",
        "        texts = cached_data['texts']\n",
        "        tokenizer = cached_data['tokenizer']\n",
        "        tokens = cached_data['tokens']\n",
        "        config.vocab_size = tokenizer.vocab_size  # Update model config\n",
        "\n",
        "        print(f\"Loaded {len(texts)} documents and {len(tokens):,} tokens from cache\")\n",
        "        return texts, tokenizer, tokens\n",
        "\n",
        "    # Cache not found; will process data from scratch\n",
        "    print(\"No cached data found; processing new dataset...\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # 2. Load and configure tokenizer\n",
        "    # ----------------------------\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
        "\n",
        "    # Ensure pad token exists for batch padding; if not, use EOS token\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # ----------------------------\n",
        "    # 3. Stream dataset to avoid memory overload\n",
        "    # ----------------------------\n",
        "    dataset = load_dataset(\n",
        "        \"HuggingFaceTB/smollm-corpus\",\n",
        "        \"cosmopedia-v2\",\n",
        "        split=\"train\",\n",
        "        streaming=True\n",
        "    )\n",
        "\n",
        "    texts = []\n",
        "    for i, item in enumerate(dataset):\n",
        "        if i >= config.num_documents:\n",
        "            break\n",
        "        # Truncate each document to 3000 characters to reduce memory usage\n",
        "        texts.append(item[\"text\"][:3000])\n",
        "\n",
        "    print(f\"Loaded {len(texts)} documents for processing\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # 4. Tokenize all documents\n",
        "    # ----------------------------\n",
        "    print(\"Tokenizing documents into model-compatible token IDs...\")\n",
        "    all_tokens = []\n",
        "    for text in tqdm(texts, desc=\"Tokenizing\"):\n",
        "        # Encode text without special tokens to maintain uniform tokenization\n",
        "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "    # ----------------------------\n",
        "    # 5. Truncate total tokens to fit memory and model constraints\n",
        "    # ----------------------------\n",
        "    tokens = all_tokens[:config.max_tokens]\n",
        "    print(f\"Final token count: {len(tokens):,}\")\n",
        "    config.vocab_size = tokenizer.vocab_size  # Update config with tokenizer vocab size\n",
        "\n",
        "    # ----------------------------\n",
        "    # 6. Save processed data to cache for future runs\n",
        "    # ----------------------------\n",
        "    cached_data = {\n",
        "        'texts': texts,\n",
        "        'tokenizer': tokenizer,\n",
        "        'tokens': tokens\n",
        "    }\n",
        "\n",
        "    with open(cache_file, 'wb') as f:\n",
        "        pickle.dump(cached_data, f)\n",
        "\n",
        "    print(f\"Processed data cached to {cache_file} for future use\")\n",
        "\n",
        "    return texts, tokenizer, tokens\n"
      ],
      "metadata": {
        "id": "ZUJ1Ehh2d-Pn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. TextTokenDataset: Sequential Token Dataset for Language Modeling**\n",
        "\n",
        "The `TextTokenDataset` class provides a simple and efficient way to prepare tokenized text data for training autoregressive language models. It transforms a flat list of token IDs into overlapping input-target sequences suitable for next-token prediction.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "* Converts a flat token list into sequences of a fixed length (`seq_len`), ready for model input.\n",
        "* Returns `(x, y)` pairs where `x` is the input sequence and `y` is the corresponding next-token targets.\n",
        "* Supports efficient slicing without duplicating data in memory.\n",
        "* Compatible with PyTorch `DataLoader` for batching and shuffling.\n",
        "* Handles corpus edges gracefully by computing `len(dataset)` as the number of available sequences."
      ],
      "metadata": {
        "id": "aB-w487phQm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextTokenDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for sequential tokenized text.\n",
        "\n",
        "    This dataset converts a flat list of tokens into overlapping\n",
        "    input-target sequences suitable for language model training.\n",
        "\n",
        "    Attributes:\n",
        "        tokens (List[int]): The complete tokenized text corpus.\n",
        "        seq_len (int): Length of each input sequence (default: 512).\n",
        "\n",
        "    Each item returns a pair (x, y):\n",
        "        x: Tensor of token IDs for the input sequence.\n",
        "        y: Tensor of token IDs for the next-token targets (shifted by 1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokens: List[int], seq_len: int = 512):\n",
        "        self.tokens = tokens\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        # Total number of sequences available in the dataset\n",
        "        return max(0, len(self.tokens) - self.seq_len)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a single training example.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Starting index of the sequence.\n",
        "\n",
        "        Returns:\n",
        "            x (torch.LongTensor): Input sequence of length `seq_len`.\n",
        "            y (torch.LongTensor): Target sequence (input shifted by 1).\n",
        "        \"\"\"\n",
        "        x = torch.tensor(self.tokens[idx:idx + self.seq_len], dtype=torch.long)\n",
        "        y = torch.tensor(self.tokens[idx + 1:idx + self.seq_len + 1], dtype=torch.long)\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "YS-JGH9ghQwi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Rotary Position Embeddings (RoPE)**\n",
        "\n",
        "Rotary Position Embeddings (RoPE) provide a continuous, rotation-based method for encoding positional information in transformer models. Unlike fixed sinusoidal or learned positional embeddings, RoPE applies **2D rotations to token embeddings based on their position**, allowing the model to naturally generalize to sequences longer than those seen during training.\n",
        "\n",
        "#### ***Key Benefits:***\n",
        "\n",
        "* **Long-range generalization:** Works well for sequences longer than the training length.\n",
        "* **Seamless integration:** Encodes position directly in the attention mechanism via rotation matrices.\n",
        "* **Preserves relative distances:** Positional information is embedded in a way that maintains token-to-token relationships, improving context modeling.\n",
        "\n",
        "### **RoPE (Rotary Positional Embedding) Math**\n",
        "\n",
        "$$\n",
        "\\text{For a token embedding } x \\in \\mathbb{R}^d \\text{ at position } p,\n",
        "\\text{ split } x \\text{ into pairs } (x_{2i}, x_{2i+1}) \\text{ and apply:}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} y_{2i} \\\\ y_{2i+1} \\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\cos \\theta_{p,i} & -\\sin \\theta_{p,i} \\\\\n",
        "\\sin \\theta_{p,i} & \\cos \\theta_{p,i}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix} x_{2i} \\\\ x_{2i+1} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{where } \\theta_{p,i} = p \\cdot \\omega_i, \\quad \\text{with} \\quad \\omega_i = \\frac{1}{10000^{2i/d}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "i = 0, 1, \\dots, \\frac{d}{2}-1, \\quad d \\text{ is the embedding dimension.}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{This rotates each pair of embedding dimensions by an angle proportional to position, preserving the vector norm.}\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8JMP3GLj6uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Rotary(nn.Module):\n",
        "    \"\"\"\n",
        "    Rotary Positional Embeddings (RoPE) for transformer attention.\n",
        "\n",
        "    This module encodes relative positional information by applying\n",
        "    sinusoidal rotations to the query/key embeddings in the attention mechanism.\n",
        "\n",
        "    Conceptually:\n",
        "        - Each embedding vector is split into two halves.\n",
        "        - These halves are rotated in a 2D plane according to a precomputed\n",
        "          sine and cosine based on the token's position and frequency.\n",
        "        - This effectively introduces a continuous, relative positional bias\n",
        "          without requiring absolute positional embeddings.\n",
        "\n",
        "    Advantages:\n",
        "        - Captures relative positions naturally.\n",
        "        - Works well with long sequences.\n",
        "        - Can be applied directly to Q/K in multi-head attention.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Dimensionality of the input embeddings (must be divisible by 4).\n",
        "        max_seq_len (int): Maximum sequence length to precompute rotation matrices.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, max_seq_len: int):\n",
        "        super().__init__()\n",
        "        assert dim % 2 == 0, \"Head dim must be divisible by 2\"\n",
        "        half_dim = dim // 2\n",
        "\n",
        "        # Frequencies\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, half_dim, dtype=torch.float32) / half_dim))\n",
        "\n",
        "        # Positions\n",
        "        positions = torch.arange(max_seq_len, dtype=torch.float32)\n",
        "\n",
        "        # Outer product -> [seq_len, half_dim]\n",
        "        theta = torch.einsum(\"i,j->ij\", positions, inv_freq)\n",
        "\n",
        "        # Cache sin/cos\n",
        "        self.register_buffer(\"cos_cached\", theta.cos(), persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", theta.sin(), persistent=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # x: [batch, seq_len, n_heads, head_dim]\n",
        "        batch, seq_len, n_heads, head_dim = x.shape\n",
        "        assert head_dim % 2 == 0\n",
        "        half_dim = head_dim // 2\n",
        "\n",
        "        # Slice and expand for broadcasting: [1, seq_len, 1, half_dim]\n",
        "        cos = self.cos_cached[:seq_len, :].unsqueeze(0).unsqueeze(2)\n",
        "        sin = self.sin_cached[:seq_len, :].unsqueeze(0).unsqueeze(2)\n",
        "\n",
        "        # Split last dimension\n",
        "        x1, x2 = x[..., :half_dim], x[..., half_dim:]\n",
        "\n",
        "        # Apply RoPE rotation\n",
        "        x_rot = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
        "        return x_rot\n",
        "\n"
      ],
      "metadata": {
        "id": "x7kNyGYrnKmF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. Grouped-Query Attention (GQA) Implementation**\n",
        "\n",
        "The attention mechanism is the core of the Qwen3 model, and **Grouped-Query Attention (GQA)** is a key efficiency innovation. This section implements the attention flow while reducing computation and memory without sacrificing performance.\n",
        "\n",
        "**Key Steps in the Attention Flow:**\n",
        "\n",
        "1. **Project Queries, Keys, and Values Separately**\n",
        "   Each input embedding is projected into query (Q), key (K), and value (V) vectors using dedicated linear layers. This allows fine-grained attention computation per head.\n",
        "\n",
        "2. **Apply QK Normalization (RMSNorm)**\n",
        "   Queries and keys are normalized using RMSNorm.\n",
        "\n",
        "   * RMSNorm stabilizes the magnitude of the vectors while preserving direction.\n",
        "   * This prevents numerical instabilities during dot-product attention and improves training convergence.\n",
        "\n",
        "3. **Incorporate Rotary Position Embeddings (RoPE)**\n",
        "   Q and K vectors are rotated based on token positions.\n",
        "\n",
        "   * Encodes relative and absolute positions efficiently.\n",
        "   * Enables the model to generalize to longer sequences than it was trained on.\n",
        "\n",
        "4. **Grouped-Query Attention (GQA)**\n",
        "   Keys and values are repeated across groups of query heads.\n",
        "\n",
        "   * Reduces the number of K/V heads compared to Q heads, saving memory and computation.\n",
        "   * Maintains expressivity because each query head can attend to a shared key/value representation.\n",
        "\n",
        "5. **Compute Scaled Dot-Product Attention**\n",
        "   Attention scores are computed as the scaled dot-product of Q and K, optionally with causal masking for autoregressive generation.\n",
        "\n",
        "   * The resulting attention weights are applied to the V vectors to produce context-aware outputs.\n",
        "\n",
        "6. **Final Linear Projection**\n",
        "   The attended output from all heads is concatenated and projected back to the model dimension, ready for the next transformer block.\n",
        "\n",
        "**Grouped-Query Attention Flow (GQA)**\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& X \\in \\mathbb{R}^{B \\times L \\times D} \\quad \\text{(input embeddings)} \\\\\n",
        "& Q = \\text{Linear}_Q(X), \\quad K = \\text{Linear}_K(X), \\quad V = \\text{Linear}_V(X) \\\\\n",
        "& Q' = \\text{RMSNorm}(Q), \\quad K' = \\text{RMSNorm}(K) \\\\\n",
        "& Q'' = \\text{RoPE}(Q'), \\quad K'' = \\text{RoPE}(K') \\\\\n",
        "& K_\\text{gqa}, V_\\text{gqa} = \\text{RepeatKV}(K'', V, n_\\text{kv\\_groups}) \\\\\n",
        "& \\text{Attention} = \\text{Softmax}\\Bigg(\\frac{Q'' K_\\text{gqa}^\\top}{\\sqrt{d_k}}\\Bigg) V_\\text{gqa} \\\\\n",
        "& \\text{Output} = \\text{Linear}_O(\\text{Attention}) \\in \\mathbb{R}^{B \\times L \\times D}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- $B$ = batch size  \n",
        "- $L$ = sequence length  \n",
        "- $D$ = hidden dimension of the model  \n",
        "- $d_k$ = dimension per attention head  \n",
        "- $n_\\text{kv\\_groups}$ = number of Key/Value groups in GQA\n",
        "\n"
      ],
      "metadata": {
        "id": "MGHFO-1uo_KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Qwen3Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Qwen3 Multi-Head Attention with Grouped-Query Attention (GQA) and Rotary Position Embeddings (RoPE).\n",
        "\n",
        "    Key Features:\n",
        "    - **Grouped-Query Attention (GQA):** Uses fewer Key/Value heads than Query heads to reduce computation while maintaining expressivity.\n",
        "    - **RMSNorm:** Root Mean Square Layer Normalization stabilizes the magnitude of query and key vectors, preventing numerical issues and helping training stability.\n",
        "    - **RoPE (Rotary Position Embeddings):** Encodes token positions using sinusoidal rotations of query/key vectors, allowing the model to generalize to longer sequences.\n",
        "    - **Causal Attention:** Ensures autoregressive behavior, so each token attends only to previous tokens.\n",
        "\n",
        "    Args:\n",
        "        config (ModelConfig): Configuration object with attention parameters (number of heads, head dimension, etc.)\n",
        "    \"\"\"\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.d_model = config.d_model\n",
        "        self.num_query_heads = config.n_heads\n",
        "        self.num_kv_heads = config.n_kv_heads\n",
        "        self.num_kv_groups = config.n_kv_groups\n",
        "        self.head_dim = config.d_k\n",
        "\n",
        "        # Linear projections for queries, keys, values\n",
        "        self.query_proj = nn.Linear(self.d_model, self.num_query_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.key_proj = nn.Linear(self.d_model, self.num_kv_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.value_proj = nn.Linear(self.d_model, self.num_kv_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.output_proj = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "        # RMSNorm for queries and keys\n",
        "        self.query_norm = nn.RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n",
        "        self.key_norm = nn.RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n",
        "\n",
        "        # Rotary positional embeddings\n",
        "        self.rotary = Rotary(self.head_dim, config.max_seq_len)\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        \"\"\"\n",
        "        Compute multi-head attention with GQA, RoPE, and RMSNorm.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Input embeddings of shape (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output of attention layer, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = hidden_states.size()\n",
        "\n",
        "        # 1. Project input embeddings into query, key, value vectors\n",
        "        queries = self.query_proj(hidden_states)\n",
        "        keys = self.key_proj(hidden_states)\n",
        "        values = self.value_proj(hidden_states)\n",
        "\n",
        "        # 2. Reshape into separate heads\n",
        "        queries = queries.view(batch_size, seq_len, self.num_query_heads, self.head_dim)\n",
        "        keys = keys.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
        "        values = values.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
        "\n",
        "        # 3. Apply RMSNorm to stabilize magnitudes\n",
        "        queries = self.query_norm(queries)  # RMSNorm scales queries to maintain consistent norm\n",
        "        keys = self.key_norm(keys)\n",
        "\n",
        "        # 4. Apply Rotary Position Embeddings\n",
        "        queries = self.rotary(queries.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
        "        keys = self.rotary(keys.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
        "\n",
        "        # 5. Transpose for attention computation: (batch, num_heads, seq_len, head_dim)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # 6. Repeat K and V heads for Grouped-Query Attention\n",
        "        keys = repeat_kv(keys, self.num_kv_groups)\n",
        "        values = repeat_kv(values, self.num_kv_groups)\n",
        "\n",
        "        # 7. Compute scaled dot-product attention with causal masking\n",
        "        attention_output = F.scaled_dot_product_attention(\n",
        "            queries, keys, values, is_causal=True, dropout_p=self.dropout if self.training else 0.0\n",
        "        )\n",
        "\n",
        "        # 8. Reshape back to (batch, seq_len, d_model)\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        # 9. Final linear projection\n",
        "        return self.output_proj(attention_output)\n"
      ],
      "metadata": {
        "id": "rLorQLKpo_Sx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10. SwiGLU Feed-Forward Network (FFN)**\n",
        "\n",
        "SwiGLU (Swish-Gated Linear Unit) is a modern feed-forward network activation that combines the **Swish** activation with a **gating mechanism (GLU)**. Compared to traditional activations like ReLU or GELU, SwiGLU improves expressivity, gradient flow, and selective feature propagation, making it well-suited for transformer-style models such as Qwen3.\n",
        "\n",
        "Conceptually, the SwiGLU operation can be thought of as:\n",
        "\n",
        "```\n",
        "output = gate(x) * value(x)\n",
        "```\n",
        "\n",
        "Or, in an intuitive analogy:\n",
        "\n",
        "```\n",
        "light = brightness_control × light_source\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "* `gate(x)` (brightness_control) determines **which features are allowed to pass through**.\n",
        "* `value(x)` (light_source) represents the **candidate features or information**.\n",
        "* Multiplying them element-wise allows the network to **selectively amplify or suppress information**, improving learning dynamics and overall model performance.\n",
        "\n",
        "Absolutely! Here's a concise MathJax version you can include in your README to visualize the SwiGLU gating mechanism:\n",
        "\n",
        "---\n",
        "\n",
        "### **SwiGLU Operation (Mathematical Form)**\n",
        "\n",
        "$$\n",
        "\\text{Output} = \\text{Gate}(x) \\odot \\text{Value}(x)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\text{Gate}(x) = \\text{Swish}(W_g x), \\quad\n",
        "\\text{Value}(x) = W_v x\n",
        "$$\n",
        "\n",
        "* $(W_g)$ and $(W_v)$ are learnable projection matrices.\n",
        "* $(\\odot)$ denotes element-wise multiplication.\n",
        "* Swish activation, with $(\\sigma)$ the sigmoid function.: $$(\\text{Swish}(z) = z \\cdot \\sigma(z))$$"
      ],
      "metadata": {
        "id": "huwY2IuIt_zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwiGLUFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    SwiGLU Feed-Forward Network (FFN)\n",
        "\n",
        "    SwiGLU (Swish-Gated Linear Unit) is a modern variant of the feed-forward\n",
        "    network used in transformers. It combines the Swish activation with a gating\n",
        "    mechanism (GLU) to improve expressivity and gradient flow compared to ReLU or GELU.\n",
        "\n",
        "    Conceptually:\n",
        "        output = gate(x) * value(x)\n",
        "    Think of it like:\n",
        "        light = brightness_control × light_source\n",
        "\n",
        "    Gating Mechanism Explanation:\n",
        "    ---------------------------------\n",
        "    - The gate pathway controls which elements of the \"value\" pathway are allowed\n",
        "      to pass through.\n",
        "    - gate(x) = Swish(W_gate * x) produces a vector with values typically in [0, 1]\n",
        "      that act like \"valves\" on the information flow.\n",
        "    - value(x) = W_up * x represents the candidate activations (information to propagate).\n",
        "    - By multiplying gate(x) * value(x) element-wise, the network can selectively\n",
        "      suppress or amplify features dynamically based on the input.\n",
        "    - This improves gradient flow, prevents saturation, and allows the FFN to model\n",
        "      richer interactions compared to traditional activations like ReLU.\n",
        "\n",
        "    Input:\n",
        "        x: Tensor of shape (batch_size, seq_len, d_model)\n",
        "\n",
        "    Output:\n",
        "        Tensor of shape (batch_size, seq_len, d_model)\n",
        "        Same shape as input, suitable for residual connections in transformer blocks.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # Project input to feed-forward dimension for the gating mechanism\n",
        "        self.gate_proj = nn.Linear(d_model, d_ff, bias=False)\n",
        "\n",
        "        # Linear layer that produces the \"value\" path\n",
        "        self.up_proj = nn.Linear(d_model, d_ff, bias=False)\n",
        "\n",
        "        # Linear layer to project back to model dimension\n",
        "        self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass:\n",
        "        1. Compute gate values using Swish (F.silu)\n",
        "        2. Multiply gate values with the \"value\" projection\n",
        "        3. Apply dropout for regularization\n",
        "        4. Project back to model dimension\n",
        "        \"\"\"\n",
        "        # Compute gate activations\n",
        "        gate_activations = F.silu(self.gate_proj(x))\n",
        "\n",
        "        # Compute value projections\n",
        "        value_activations = self.up_proj(x)\n",
        "\n",
        "        # Element-wise gating\n",
        "        gated_output = gate_activations * value_activations\n",
        "\n",
        "        # Apply dropout and project back\n",
        "        return self.down_proj(self.dropout(gated_output))\n"
      ],
      "metadata": {
        "id": "sQFpiilbt_9T"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **11. Transformer Block**\n",
        "\n",
        "Each Transformer block integrates multi-head attention and a feed-forward network (FFN), connected through residual pathways and pre-normalization layers to ensure stable gradient flow during deep training.\n",
        "\n",
        "Unlike traditional implementations that use LayerNorm, this model employs RMSNorm (Root Mean Square Normalization), which normalizes activations based on their RMS magnitude rather than their mean and variance. This approach improves numerical stability, training efficiency, and scalability, especially in large or low-precision models.\n"
      ],
      "metadata": {
        "id": "APdqPHgY4V0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Block with Grouped-Query Attention (GQA) and SwiGLU Feed-Forward Network.\n",
        "\n",
        "    This block integrates attention and feed-forward computation in a pre-normalized\n",
        "    residual architecture for stable deep training. Each block performs:\n",
        "\n",
        "        Input\n",
        "          → RMSNorm                    # Normalizes activations by their root-mean-square value\n",
        "                                       # Ensures stable feature scaling before attention\n",
        "                                       # Helps gradients stay well-conditioned in deep networks\n",
        "\n",
        "          → Multi-Head GQA (with RoPE) # Applies Grouped-Query Attention\n",
        "                                       # - Projects input into queries, keys, and values\n",
        "                                       # - Uses shared key/value projections for efficiency\n",
        "                                       # - Injects positional context using RoPE (rotational encoding)\n",
        "                                       # Outputs context-aware representations per token\n",
        "\n",
        "          → Dropout + Residual Add     # Combines attention output with the original input (residual path)\n",
        "                                       # - Dropout regularizes to prevent overfitting\n",
        "                                       # - Residual connection preserves original information flow\n",
        "\n",
        "          → RMSNorm                    # Normalizes again before feed-forward processing\n",
        "                                       # Prepares stable activations for the non-linear transformation\n",
        "\n",
        "          → SwiGLU Feed-Forward Network# Applies two-layer feed-forward MLP with SwiGLU activation\n",
        "                                       # - SwiGLU uses a gated Swish: (x * σ(W₁x))W₂\n",
        "                                       # - Expands hidden dimension (d_ff), then projects back to d_model\n",
        "                                       # - Increases representational capacity and non-linearity\n",
        "\n",
        "          → Dropout + Residual Add     # Adds the feed-forward output back to the residual stream\n",
        "                                       # - Dropout regularizes the FFN output\n",
        "                                       # - Residual path ensures smoother gradient propagation\n",
        "\n",
        "          → Output                     # Returns the updated hidden states\n",
        "                                       # Now encoded with both contextual (attention) and\n",
        "                                       # nonlinear (feed-forward) information, ready for the next block\n",
        "\n",
        "    Key Features:\n",
        "        • **Grouped-Query Attention (GQA)** — shares key/value projections across query groups,\n",
        "          reducing memory and computation while preserving representational power.\n",
        "        • **RoPE (Rotary Positional Embedding)** — encodes absolute and relative positional\n",
        "          information directly in query/key space via rotational transformations.\n",
        "        • **SwiGLU Feed-Forward Network** — uses a gated Swish activation for enhanced\n",
        "          nonlinearity and expressivity over ReLU or GELU.\n",
        "        • **RMSNorm (Root Mean Square Normalization)** — normalizes activations by their RMS\n",
        "          magnitude, improving numerical stability and training efficiency compared to LayerNorm.\n",
        "        • **Pre-Norm Residual Structure** — applies normalization before sublayers to ensure\n",
        "          gradient flow and stable convergence in deep transformer stacks.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model)\n",
        "    \"\"\"\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        # -----------------------------\n",
        "        # Attention Block Components\n",
        "        # -----------------------------\n",
        "\n",
        "        # Multi-head attention with Grouped-Query Attention (GQA)\n",
        "        # - QK-normalization stabilizes the queries and keys\n",
        "        # - RoPE encodes positional information\n",
        "        self.attention = Qwen3Attention(config)\n",
        "\n",
        "        # RMSNorm applied before attention (pre-norm)\n",
        "        # - Normalizes input across feature dimension\n",
        "        # - Helps with gradient stability in deep networks\n",
        "        self.attn_norm = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "\n",
        "        # -----------------------------\n",
        "        # Feed-Forward Block Components\n",
        "        # -----------------------------\n",
        "\n",
        "        # Feed-forward network using SwiGLU activation\n",
        "        # - SwiGLU = Swish gating mechanism × linear projection\n",
        "        # - Improves expressivity compared to ReLU/GELU\n",
        "        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n",
        "\n",
        "        # RMSNorm applied before feed-forward (pre-norm)\n",
        "        # - Stabilizes input to FFN\n",
        "        self.ffn_norm = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "\n",
        "        # Dropout applied to residual connections for regularization\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the Transformer block.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        # -----------------------------\n",
        "        # 1. Attention Block\n",
        "        # -----------------------------\n",
        "\n",
        "        # Normalize input features before attention\n",
        "        normed_input = self.attn_norm(x)  # shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Compute attention output:\n",
        "        # - Queries, keys, values are projected and normalized\n",
        "        # - Rotary embeddings (RoPE) add positional information\n",
        "        # - Grouped-Query Attention (GQA) allows KV sharing for efficiency\n",
        "        attention_output = self.attention(normed_input)  # shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Apply residual connection and dropout:\n",
        "        # - Residual preserves original input information\n",
        "        # - Dropout helps prevent overfitting\n",
        "        x = x + self.dropout(attention_output)\n",
        "\n",
        "        # -----------------------------\n",
        "        # 2. Feed-Forward Block\n",
        "        # -----------------------------\n",
        "\n",
        "        # Normalize the output of attention block before feed-forward\n",
        "        normed_residual = self.ffn_norm(x)  # shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Compute feed-forward output:\n",
        "        # - SwiGLU applies gating mechanism (Swish × linear)\n",
        "        # - Expands features to d_ff dimension and projects back to d_model\n",
        "        ffn_output = self.feed_forward(normed_residual)  # shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Apply residual connection and dropout:\n",
        "        # - Preserves input to feed-forward network\n",
        "        # - Dropout regularizes FFN output\n",
        "        x = x + self.dropout(ffn_output)\n",
        "\n",
        "        # Output has same shape as input, ready for next Transformer block\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "JfEF4JDK4V-1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12. Complete Language Model**\n",
        "\n",
        "This stage integrates all the components into a cohesive **Transformer-based language model**. The model encodes token sequences, processes them through stacked attention and feed-forward layers, and outputs logits for next-token prediction.\n",
        "\n",
        "**Architecture Overview:**\n",
        "\n",
        "* **Token Embeddings:** Convert discrete tokens into continuous vector representations.\n",
        "* **Positional Dropout:** Adds regularization to improve generalization and robustness.\n",
        "* **Stacked Transformer Blocks:** Each block refines contextual understanding through attention and non-linear transformations.\n",
        "* **Final RMSNorm & Output Projection:** Normalize the final representations and project them back to the vocabulary dimension.\n",
        "* **Weight Tying:** Shares weights between input embeddings and output projection to reduce parameters and improve consistency between encoding and decoding spaces.\n"
      ],
      "metadata": {
        "id": "3xXHpnFIj_jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MinimalLLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Minimal Transformer-based Language Model.\n",
        "\n",
        "    This module implements a compact, decoder-only Transformer architecture\n",
        "    capturing the essential design of modern large language models (LLMs).\n",
        "    It includes token embeddings, stacked self-attention and feed-forward\n",
        "    blocks, normalization layers, and a tied output projection head.\n",
        "\n",
        "    **Model Overview**\n",
        "    1. **Token Embedding:** Converts input token IDs into dense vector representations.\n",
        "    2. **Positional Dropout:** Adds stochastic regularization to embeddings before encoding.\n",
        "    3. **Transformer Stack:** Sequentially applies multiple attention + feed-forward\n",
        "       blocks to model contextual dependencies and hierarchical semantics.\n",
        "    4. **Final RMSNorm:** Normalizes the hidden states for improved numerical stability.\n",
        "    5. **Output Projection (Weight-Tied):** Maps the final hidden states back to\n",
        "       vocabulary logits using shared weights from the input embeddings.\n",
        "\n",
        "    Designed for clarity and educational readability while preserving\n",
        "    architectural fidelity to modern decoder-only LLMs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # -------------------------------------------------------\n",
        "        # 1. Embedding Layer\n",
        "        # -------------------------------------------------------\n",
        "\n",
        "        # Token embeddings map discrete vocabulary indices to dense vectors.\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "\n",
        "        # Optional dropout on embeddings — helps regularize positional patterns\n",
        "        self.position_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # -------------------------------------------------------\n",
        "        # 2. Transformer Backbone\n",
        "        # -------------------------------------------------------\n",
        "\n",
        "        # Stack of N identical Transformer blocks (each with attention + FFN)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(config) for _ in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        # -------------------------------------------------------\n",
        "        # 3. Output Normalization and Projection\n",
        "        # -------------------------------------------------------\n",
        "\n",
        "        # Final RMSNorm stabilizes activations before output projection.\n",
        "        self.norm = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "\n",
        "        # Dropout before logits for additional regularization.\n",
        "        self.output_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # -------------------------------------------------------\n",
        "        # 4. Output Head (Weight Tying)\n",
        "        # -------------------------------------------------------\n",
        "\n",
        "        # Linear layer projects hidden states back to vocabulary space.\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying — reuse token embedding weights for output projection.\n",
        "        # This reduces parameter count and improves generalization,\n",
        "        # ensuring that the input and output token spaces share semantics.\n",
        "        self.lm_head.weight = self.token_embedding.weight\n",
        "\n",
        "        # -------------------------------------------------------\n",
        "        # 5. Parameter Initialization\n",
        "        # -------------------------------------------------------\n",
        "\n",
        "        # Initialize weights with small Gaussian noise for stable training.\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Applies standard normal initialization to Linear and Embedding layers.\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the minimal LLM.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Token indices of shape (batch_size, seq_len).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Logits of shape (batch_size, seq_len, vocab_size),\n",
        "            representing unnormalized probabilities for each vocabulary token.\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Token Embedding + Scaling\n",
        "        # Scale embeddings by sqrt(d_model) to keep variance consistent.\n",
        "        x = self.token_embedding(x) * math.sqrt(self.config.d_model)\n",
        "\n",
        "        # 2. Embedding Dropout\n",
        "        x = self.position_dropout(x)\n",
        "\n",
        "        # 3. Transformer Stack\n",
        "        # Sequentially apply each attention–feedforward block.\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # 4. Final Normalization + Dropout\n",
        "        x = self.norm(x)\n",
        "        x = self.output_dropout(x)\n",
        "\n",
        "        # 5. Output Projection (shared weights)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "XOHwvbsCj_uP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **13. Evaluation Function**\n",
        "\n",
        "During training, it is essential to periodically assess the model’s performance on a **validation dataset**.\n",
        "This evaluation function computes the following key metrics:\n",
        "\n",
        "* **Loss:** Average token-level cross-entropy, measuring how well the model predicts the next token.\n",
        "* **Accuracy:** Fraction of tokens correctly predicted, providing an intuitive performance measure.\n",
        "* **Perplexity:** Exponential of the loss, commonly used to quantify uncertainty in language modeling — lower perplexity indicates better predictive performance.\n",
        "\n",
        "The function is designed to be **efficient** by:\n",
        "\n",
        "* Disabling gradient computation to save memory and computation time.\n",
        "* Using **mixed precision** if enabled for faster evaluation.\n",
        "* Limiting the number of evaluation steps (`config.eval_steps`) to control runtime on large datasets."
      ],
      "metadata": {
        "id": "pxVuZPgpmY6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model: nn.Module, val_loader: DataLoader, config: ModelConfig):\n",
        "    \"\"\"\n",
        "    Evaluate a trained language model on a validation dataset.\n",
        "\n",
        "    **Evaluation Steps:**\n",
        "    1. Switch the model to evaluation mode (disables dropout, etc.)\n",
        "    2. Iterate over validation batches (limited by `config.eval_steps`)\n",
        "    3. Move inputs and targets to the model device (CPU/GPU)\n",
        "    4. Forward pass through the model (mixed precision if enabled)\n",
        "    5. Compute token-level cross-entropy loss\n",
        "    6. Accumulate total loss, correct predictions, and token counts\n",
        "    7. Compute average metrics: loss, accuracy, and perplexity\n",
        "    8. Restore the model to training mode\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            'val_loss': float,\n",
        "            'val_accuracy': float,\n",
        "            'val_perplexity': float\n",
        "        }\n",
        "    \"\"\"\n",
        "    # -----------------------------\n",
        "    # 1. Switch to evaluation mode\n",
        "    # -----------------------------\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize running totals\n",
        "    total_loss = 0.0          # Total cross-entropy loss\n",
        "    total_tokens = 0          # Total number of tokens processed\n",
        "    total_correct = 0         # Number of correctly predicted tokens\n",
        "\n",
        "    # Determine which device the model is on (CPU/GPU)\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2. Iterate over validation batches\n",
        "    # -----------------------------\n",
        "    with torch.no_grad():  # Disable gradients for efficiency\n",
        "        for step, (x, y) in enumerate(val_loader):\n",
        "\n",
        "            # Limit evaluation to a maximum number of steps\n",
        "            if step >= config.eval_steps:\n",
        "                break\n",
        "\n",
        "            # -----------------------------\n",
        "            # 3. Move batch to device\n",
        "            # -----------------------------\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # -----------------------------\n",
        "            # 4. Forward pass with mixed precision\n",
        "            # -----------------------------\n",
        "            with autocast(enabled=config.use_amp):\n",
        "                logits = model(x)\n",
        "\n",
        "                # -----------------------------\n",
        "                # 5. Compute cross-entropy loss\n",
        "                # -----------------------------\n",
        "                # Flatten batch and sequence dimensions for proper token-wise loss\n",
        "                loss = F.cross_entropy(\n",
        "                    logits.view(-1, config.vocab_size),\n",
        "                    y.view(-1),\n",
        "                    reduction='mean'\n",
        "                )\n",
        "\n",
        "            # -----------------------------\n",
        "            # 6. Accumulate metrics\n",
        "            # -----------------------------\n",
        "            batch_tokens = y.numel()              # Number of tokens in this batch\n",
        "            total_loss += loss.item() * batch_tokens  # Weighted sum of loss\n",
        "            total_tokens += batch_tokens\n",
        "\n",
        "            # Token-level predictions (argmax over vocab dimension)\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            total_correct += (predictions == y).sum().item()\n",
        "\n",
        "    # -----------------------------\n",
        "    # 7. Compute average metrics\n",
        "    # -----------------------------\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    accuracy = total_correct / total_tokens\n",
        "    perplexity = math.exp(min(avg_loss, 20))  # Clamp to prevent overflow\n",
        "\n",
        "    # -----------------------------\n",
        "    # 8. Restore training mode\n",
        "    # -----------------------------\n",
        "    model.train()\n",
        "\n",
        "    return {\n",
        "        'val_loss': avg_loss,\n",
        "        'val_accuracy': accuracy,\n",
        "        'val_perplexity': perplexity\n",
        "    }\n"
      ],
      "metadata": {
        "id": "YOCeULJslglV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14. Optimizer Setup**\n",
        "\n",
        "The model employs a **hybrid optimization strategy** to leverage the strengths of two different optimizers:\n",
        "\n",
        "* **Muon Optimizer:** Applied to 2D parameters (primarily linear layer weights in attention and feed-forward blocks).\n",
        "\n",
        "  * Performs **momentum-orthogonalized updates** to stabilize training and improve convergence for large weight matrices.\n",
        "\n",
        "* **AdamW:** Applied to all remaining parameters (embeddings, biases, normalization layers).\n",
        "\n",
        "  * Provides standard **adaptive learning rate optimization** and weight decay regularization.\n",
        "\n",
        "**Step-by-Step Flow:**\n",
        "\n",
        "```\n",
        "Model Parameters\n",
        "      ↓\n",
        "Separate by type\n",
        "      ├─ 2D Linear Weights → Muon Optimizer → Orthogonalized Momentum Updates\n",
        "      └─ Embeddings / Biases / Norms → AdamW → Adaptive Learning Rate Updates\n",
        "      ↓\n",
        "Training Updates Applied to Model\n",
        "```\n",
        "\n",
        "This hybrid approach allows the model to benefit from **Muon’s stability for large weight matrices** while **AdamW efficiently handles smaller or non-square parameters**, combining the advantages of both optimizers in a single training run.\n"
      ],
      "metadata": {
        "id": "QBwkuGT7nNHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_muon_optimizer(model: nn.Module, config: ModelConfig):\n",
        "    \"\"\"\n",
        "    Configure a hybrid optimizer setup combining Muon and AdamW.\n",
        "\n",
        "    Certain parameters (mostly weight matrices in linear layers) are optimized\n",
        "    using the Muon optimizer, which applies momentum orthogonalization for better\n",
        "    convergence and stability. The remaining parameters (embeddings, biases, norms)\n",
        "    are optimized using standard AdamW.\n",
        "\n",
        "    Steps:\n",
        "    1. Separate model parameters into Muon vs AdamW groups.\n",
        "    2. Print parameter counts for transparency.\n",
        "    3. Instantiate Muon optimizer for applicable parameters.\n",
        "    4. Instantiate AdamW optimizer for remaining parameters.\n",
        "    5. Return both optimizers in a list for joint training.\n",
        "    \"\"\"\n",
        "\n",
        "    # Lists to hold parameters for each optimizer\n",
        "    muon_params = []  # Typically weight matrices in linear layers\n",
        "    adamw_params = [] # Embeddings, biases, normalization layers, etc.\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1. Separate parameters\n",
        "    # -----------------------------\n",
        "    for name, param in model.named_parameters():\n",
        "        # Criteria for Muon optimizer:\n",
        "        # - 2D tensors (linear layer weights)\n",
        "        # - Not embeddings or normalization layers\n",
        "        # - Must require gradients\n",
        "        if (param.ndim == 2 and\n",
        "            'token_embedding' not in name and\n",
        "            'norm' not in name and\n",
        "            param.requires_grad):\n",
        "            muon_params.append(param)\n",
        "        else:\n",
        "            adamw_params.append(param)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2. Print parameter counts\n",
        "    # -----------------------------\n",
        "    # Provides quick transparency for debugging and reporting\n",
        "    print(f\"  Muon parameters: {sum(p.numel() for p in muon_params):,}\")\n",
        "    print(f\"  AdamW parameters: {sum(p.numel() for p in adamw_params):,}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3. Instantiate Muon optimizer\n",
        "    # -----------------------------\n",
        "    # Momentum-orthogonalized updates for linear weight matrices\n",
        "    muon_optimizer = Muon(\n",
        "        muon_params,\n",
        "        lr=config.muon_lr,      # Muon-specific learning rate\n",
        "        momentum=0.95\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4. Instantiate AdamW optimizer\n",
        "    # -----------------------------\n",
        "    # Standard AdamW for embeddings, biases, and normalization layers\n",
        "    adamw_optimizer = torch.optim.AdamW(\n",
        "        adamw_params,\n",
        "        lr=config.muon_lr * 0.1,  # Smaller learning rate than Muon\n",
        "        weight_decay=config.weight_decay\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5. Return both optimizers\n",
        "    # -----------------------------\n",
        "    return [muon_optimizer, adamw_optimizer]\n"
      ],
      "metadata": {
        "id": "35qqFVUwnNOq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **15. Training Loop**\n",
        "\n",
        "The training loop orchestrates the end-to-end learning process for the language model.\n",
        "It integrates **forward/backward passes, gradient accumulation, optimizer updates, learning rate scheduling, logging, evaluation, and checkpointing**.\n",
        "\n",
        "### **Step-by-Step Flow**\n",
        "\n",
        "```\n",
        "Initialize Model → Move to Device → Setup Optimizers & Schedulers → Configure Mixed Precision\n",
        "      ↓\n",
        "Iterate over Training Batches\n",
        "      ↓\n",
        "Forward Pass → Compute Cross-Entropy Loss → Scale Loss (if using AMP)\n",
        "      ↓\n",
        "Backward Pass → Gradient Accumulation\n",
        "      ↓\n",
        "Gradient Clipping → Optimizer Step → Scheduler Step → Reset Gradients\n",
        "      ↓\n",
        "Periodic Logging → Loss / Accuracy / Perplexity\n",
        "      ↓\n",
        "Periodic Validation → Evaluate on Val Set → Save Best Model\n",
        "      ↓\n",
        "Repeat Until Max Steps Reached\n",
        "      ↓\n",
        "Final Evaluation → Save Final Model Checkpoint\n",
        "```\n",
        "\n",
        "### **Key Features**\n",
        "\n",
        "* **Hybrid Optimizer:** Uses **Muon** for 2D weight matrices and **AdamW** for remaining parameters.\n",
        "* **Mixed Precision (AMP):** Optional acceleration of training with minimal loss in numerical precision.\n",
        "* **Gradient Accumulation:** Allows effective large-batch training even with limited GPU memory.\n",
        "* **Learning Rate Scheduling:** Linear warmup followed by cosine decay for stable convergence.\n",
        "* **Logging & Metrics:** Tracks token-level loss, accuracy, and perplexity every few steps.\n",
        "* **Checkpointing:** Saves both the **best validation model** and the **final model**.\n",
        "\n",
        "This loop ensures **efficient, stable, and reproducible training** of the MinimalLLM language model.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also make a **visual “full training pipeline” diagram** that combines **embedding → Transformer blocks → optimizer → evaluation → checkpointing** in a single cohesive flow, so it matches all your previous README sections visually.\n",
        "\n",
        "Do you want me to do that next?\n"
      ],
      "metadata": {
        "id": "y1SY2gnJrnBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(config: ModelConfig, train_loader: DataLoader, val_loader: DataLoader):\n",
        "    \"\"\"\n",
        "    Train a MinimalLLM model using a hybrid Muon + AdamW optimizer with optional mixed-precision.\n",
        "\n",
        "    This function handles all steps of training from initialization to final evaluation and model saving.\n",
        "    It is designed for clarity, stability, and reproducibility, with advanced features such as:\n",
        "        - Gradient accumulation for effective large-batch training.\n",
        "        - Mixed-precision (AMP) for faster computation with minimal memory footprint.\n",
        "        - Hybrid optimizer setup (Muon for 2D weights, AdamW for others).\n",
        "        - Cosine learning rate schedule with linear warmup.\n",
        "        - Gradient clipping to avoid exploding gradients.\n",
        "        - Periodic evaluation and best model checkpointing.\n",
        "        - Logging of loss, token-level accuracy, and perplexity for monitoring.\n",
        "\n",
        "    -----------------------------\n",
        "    Training Steps Overview\n",
        "    -----------------------------\n",
        "    1. Initialize the MinimalLLM model and set a random seed for reproducibility.\n",
        "    2. Move the model to the correct device (GPU if available, otherwise CPU).\n",
        "    3. Count total trainable parameters for reference.\n",
        "    4. Setup the hybrid optimizers:\n",
        "        - Muon optimizer for 2D weight matrices (attention and feed-forward layers)\n",
        "        - AdamW optimizer for embeddings, biases, and normalization layers\n",
        "    5. Configure learning rate schedulers with linear warmup + cosine decay.\n",
        "    6. Setup mixed-precision training (if enabled via config.use_amp) using torch.cuda.amp.GradScaler.\n",
        "    7. Iterate through training batches:\n",
        "        a) Move input (x) and target (y) tensors to device.\n",
        "        b) Forward pass through the model to get logits.\n",
        "        c) Compute cross-entropy loss and normalize it for gradient accumulation.\n",
        "        d) Backward pass:\n",
        "            - Use scaler.scale() for AMP\n",
        "            - Accumulate gradients over multiple steps if gradient_accumulation_steps > 1\n",
        "        e) Gradient clipping to avoid exploding gradients.\n",
        "        f) Optimizer step and zero gradients after accumulation.\n",
        "        g) Step the learning rate schedulers.\n",
        "        h) Log training metrics every few steps:\n",
        "            - Loss\n",
        "            - Token-level accuracy\n",
        "            - Perplexity\n",
        "            - Learning rate\n",
        "        i) Periodically evaluate on validation set and save best model checkpoint.\n",
        "    8. After completing all training steps:\n",
        "        - Perform final evaluation on validation set.\n",
        "        - Save the final model checkpoint.\n",
        "\n",
        "    Args:\n",
        "        config (ModelConfig): Configuration object containing model and training hyperparameters.\n",
        "        train_loader (DataLoader): PyTorch DataLoader for training dataset.\n",
        "        val_loader (DataLoader): PyTorch DataLoader for validation dataset.\n",
        "\n",
        "    Returns:\n",
        "        model (nn.Module): Trained MinimalLLM model.\n",
        "        final_eval (dict): Dictionary containing final evaluation metrics:\n",
        "            - val_loss\n",
        "            - val_accuracy\n",
        "            - val_perplexity\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n🚀 Training Small model with Muon optimizer\")\n",
        "\n",
        "     # -----------------------------\n",
        "    # 1. Initialize model\n",
        "    # -----------------------------\n",
        "    set_seed(42)  # Fix random seed for reproducibility of results\n",
        "    model = MinimalLLM(config)  # Instantiate the MinimalLLM model with given configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Select GPU if available\n",
        "    model = model.to(device)  # Move model parameters to selected device\n",
        "\n",
        "    # Count and print total number of trainable parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"  📊 Total parameters: {total_params:,}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2. Setup optimizers and schedulers\n",
        "    # -----------------------------\n",
        "    optimizers = setup_muon_optimizer(model, config)  # Create Muon + AdamW hybrid optimizers\n",
        "\n",
        "    schedulers = []\n",
        "    for optimizer in optimizers:\n",
        "        warmup_steps = config.max_steps // 20  # Set warmup for first 5% of steps\n",
        "\n",
        "        def lr_lambda(step):\n",
        "            # Linear warmup for first `warmup_steps` steps\n",
        "            if step < warmup_steps:\n",
        "                return step / warmup_steps\n",
        "            else:\n",
        "                # Cosine decay for remaining steps\n",
        "                progress = (step - warmup_steps) / (config.max_steps - warmup_steps)\n",
        "                return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)  # Attach scheduler\n",
        "        schedulers.append(scheduler)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3. Setup mixed precision (optional)\n",
        "    # -----------------------------\n",
        "    scaler = GradScaler() if config.use_amp else None  # Enable AMP for faster computation if required\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4. Training loop setup\n",
        "    # -----------------------------\n",
        "    model.train()  # Set model to training mode\n",
        "    step = 0  # Initialize global step counter\n",
        "    start_time = time.time()  # Start timing the training\n",
        "    best_val_loss = float('inf')  # Initialize best validation loss for checkpointing\n",
        "\n",
        "    pbar = tqdm(total=config.max_steps, desc=\"Training\")  # Create progress bar\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5. Iterate over batches\n",
        "    # -----------------------------\n",
        "    while step < config.max_steps:\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            if step >= config.max_steps:\n",
        "                break  # Stop if max training steps reached\n",
        "\n",
        "            # Move input and target tensors to the correct device\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # -----------------------------\n",
        "            # Forward and backward pass\n",
        "            # -----------------------------\n",
        "            if config.use_amp:  # Use mixed precision\n",
        "                with autocast():  # Enable automatic mixed precision\n",
        "                    logits = model(x)  # Forward pass\n",
        "                    # Compute cross-entropy loss and normalize for gradient accumulation\n",
        "                    loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "                    loss = loss / config.gradient_accumulation_steps\n",
        "                scaler.scale(loss).backward()  # Scale loss and backpropagate\n",
        "            else:\n",
        "                logits = model(x)  # Forward pass\n",
        "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "                loss = loss / config.gradient_accumulation_steps\n",
        "                loss.backward()  # Backpropagation\n",
        "\n",
        "            # -----------------------------\n",
        "            # Optimizer step after gradient accumulation\n",
        "            # -----------------------------\n",
        "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "                # Clip gradients to prevent exploding gradients\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "                if config.use_amp:\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.unscale_(optimizer)  # Unscale gradients before optimizer step\n",
        "                        scaler.step(optimizer)  # Apply optimizer step\n",
        "                        optimizer.zero_grad()  # Reset gradients\n",
        "                    for scheduler in schedulers:\n",
        "                        scheduler.step()  # Update learning rate\n",
        "                    scaler.update()  # Update scaler for next iteration\n",
        "                else:\n",
        "                    for optimizer in optimizers:\n",
        "                        optimizer.step()  # Apply optimizer step\n",
        "                        optimizer.zero_grad()  # Reset gradients\n",
        "                    for scheduler in schedulers:\n",
        "                        scheduler.step()  # Update learning rate\n",
        "\n",
        "            # -----------------------------\n",
        "            # Logging\n",
        "            # -----------------------------\n",
        "            if step % 10 == 0:\n",
        "                with torch.no_grad():  # Disable gradient computation for metrics\n",
        "                    predictions = logits.argmax(dim=-1)  # Predicted token IDs\n",
        "                    accuracy = (predictions == y).float().mean().item()  # Token-level accuracy\n",
        "                    current_loss = loss.item() * config.gradient_accumulation_steps  # Rescale loss\n",
        "                    perplexity = math.exp(min(current_loss, 20))  # Compute perplexity\n",
        "\n",
        "                # Update progress bar with metrics\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{current_loss:.4f}',\n",
        "                    'acc': f'{accuracy:.3f}',\n",
        "                    'ppl': f'{perplexity:.1f}',\n",
        "                    'lr': f'{optimizers[0].param_groups[0][\"lr\"]:.2e}'\n",
        "                })\n",
        "\n",
        "            # -----------------------------\n",
        "            # Periodic evaluation and checkpointing\n",
        "            # -----------------------------\n",
        "            if step % config.eval_every == 0 and step > 0:\n",
        "                eval_metrics = evaluate_model(model, val_loader, config)  # Evaluate on validation set\n",
        "                print(f\"\\nStep {step}: Val Loss: {eval_metrics['val_loss']:.4f}, \"\n",
        "                      f\"Val Acc: {eval_metrics['val_accuracy']:.4f}, \"\n",
        "                      f\"Val PPL: {eval_metrics['val_perplexity']:.2f}\")\n",
        "\n",
        "                # Save checkpoint if validation loss improves\n",
        "                if eval_metrics['val_loss'] < best_val_loss:\n",
        "                    best_val_loss = eval_metrics['val_loss']\n",
        "                    torch.save({\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'config': config,\n",
        "                        'step': step,\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                        'final_metrics': eval_metrics\n",
        "                    }, 'best_model.pt')\n",
        "                    print(f\"💾 Saved best model with val_loss: {best_val_loss:.4f}\")\n",
        "\n",
        "            step += 1  # Increment global step\n",
        "            if step % 10 == 0:\n",
        "                pbar.update(10)  # Update progress bar\n",
        "\n",
        "    pbar.close()  # Close progress bar at the end of training\n",
        "\n",
        "    # -----------------------------\n",
        "    # Final evaluation and saving\n",
        "    # -----------------------------\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"  ⏱️ Training completed in {training_time:.1f} seconds\")\n",
        "\n",
        "    final_eval = evaluate_model(model, val_loader, config)  # Evaluate final model\n",
        "    print(f\"  📊 Final - Loss: {final_eval['val_loss']:.4f}, \"\n",
        "          f\"Acc: {final_eval['val_accuracy']:.4f}, PPL: {final_eval['val_perplexity']:.2f}\")\n",
        "\n",
        "    # Save final model checkpoint\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': config,\n",
        "        'step': step,\n",
        "        'final_metrics': final_eval\n",
        "    }, 'final_model.pt')\n",
        "    print(f\"💾 Saved final model to final_model.pt\")\n",
        "\n",
        "    return model, final_eval  # Return trained model and final evaluation metrics"
      ],
      "metadata": {
        "id": "KRPsAzjPrdbp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **16. Main Training Script**\n",
        "\n",
        "This script orchestrates the end-to-end training of the **MinimalLLM** model.\n",
        "\n",
        "**Step-by-Step Overview:**\n",
        "\n",
        "1. **Check System and Device**\n",
        "\n",
        "   * Detects whether GPU (CUDA) is available; prints GPU name and memory if applicable.\n",
        "\n",
        "2. **Set Random Seed**\n",
        "\n",
        "   * Ensures reproducible results for model initialization, data shuffling, and training.\n",
        "\n",
        "3. **Create Model Configuration**\n",
        "\n",
        "   * Initializes a `ModelConfig` object with architecture and training hyperparameters.\n",
        "   * Prints a summary of model size, training steps, batch size, and dataset details.\n",
        "\n",
        "4. **Load and Preprocess Data**\n",
        "\n",
        "   * Tokenizes raw text and converts it to token IDs.\n",
        "   * Wraps tokens in a PyTorch `TextTokenDataset` for easy batching.\n",
        "\n",
        "5. **Train/Validation Split**\n",
        "\n",
        "   * Splits dataset into 90% training and 10% validation.\n",
        "   * Uses a fixed random seed for reproducibility.\n",
        "\n",
        "6. **Create PyTorch DataLoaders**\n",
        "\n",
        "   * Wraps datasets in `DataLoader` for batched training and evaluation.\n",
        "   * Enables shuffling for training, disables shuffling for validation.\n",
        "\n",
        "7. **Train the Model**\n",
        "\n",
        "   * Calls `train_model()` to run the full training loop with hybrid Muon + AdamW optimizer, gradient accumulation, optional mixed precision, logging, and checkpointing.\n",
        "\n",
        "8. **Print Final Training Summary**\n",
        "\n",
        "   * Reports total training time.\n",
        "   * Displays final validation metrics: loss, token-level accuracy, and perplexity.\n",
        "\n",
        "This main script provides a reproducible and fully automated pipeline for training a small transformer-based language model on your dataset."
      ],
      "metadata": {
        "id": "_e-Rjhlb3mv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # -----------------------------\n",
        "    # 1. Check system and device\n",
        "    # -----------------------------\n",
        "    device_name = 'CUDA' if torch.cuda.is_available() else 'CPU'\n",
        "    print(f\"🔍 Device: {device_name}\")  # Show whether GPU or CPU will be used\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        # Print GPU name\n",
        "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        # Print total GPU memory in GB\n",
        "        total_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"Memory: {total_mem_gb:.1f} GB\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2. Set random seed\n",
        "    # -----------------------------\n",
        "    set_seed(42)  # Ensures reproducibility of model initialization, data shuffling, etc.\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3. Create model configuration\n",
        "    # -----------------------------\n",
        "    config = ModelConfig()  # Initialize default configuration for the Small MinimalLLM\n",
        "    print(f\"\\n📋 Model Configuration:\")\n",
        "    print(f\"   Architecture: {config.d_model}d, {config.n_layers}L, {config.n_heads}H, {config.d_ff}ff\")\n",
        "    print(f\"   Training: {config.max_steps} steps, batch size {config.batch_size}\")\n",
        "    print(f\"   Data: {config.max_tokens:,} tokens, seq_len {config.max_seq_len}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4. Load and preprocess data\n",
        "    # -----------------------------\n",
        "    # load_and_cache_data() should return:\n",
        "    # - texts: raw text strings\n",
        "    # - tokenizer: tokenizer object for encoding/decoding\n",
        "    # - tokens: list of token ids for all text\n",
        "    texts, tokenizer, tokens = load_and_cache_data(config)\n",
        "\n",
        "    # Wrap token ids in a PyTorch dataset for easy batching\n",
        "    dataset = TextTokenDataset(tokens, config.max_seq_len)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5. Train/validation split\n",
        "    # -----------------------------\n",
        "    val_size = len(dataset) // 10  # 10% for validation\n",
        "    train_size = len(dataset) - val_size  # 90% for training\n",
        "\n",
        "    # Random split with fixed seed for reproducibility\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # 6. Create PyTorch DataLoaders\n",
        "    # -----------------------------\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,  # Shuffle training data each epoch\n",
        "        num_workers=2  # Number of subprocesses for data loading\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,  # No need to shuffle validation data\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    print(f\"📊 Dataset: {len(train_dataset)} train, {len(val_dataset)} val samples\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 7. Train the model\n",
        "    # -----------------------------\n",
        "    start_time = time.time()  # Record start time for total training duration\n",
        "    model, final_metrics = train_model(config, train_loader, val_loader)  # Call training function\n",
        "    total_time = time.time() - start_time  # Compute total training time\n",
        "\n",
        "    # -----------------------------\n",
        "    # 8. Print summary of training\n",
        "    # -----------------------------\n",
        "    print(f\"\\n🎉 TRAINING COMPLETED!\")\n",
        "    print(f\"⏱️ Total time: {total_time/60:.1f} minutes\")  # Convert seconds to minutes\n",
        "    print(f\"🏆 Final Results:\")\n",
        "    print(f\"   Validation Loss: {final_metrics['val_loss']:.4f}\")  # Cross-entropy loss\n",
        "    print(f\"   Validation Accuracy: {final_metrics['val_accuracy']:.4f}\")  # Token-level accuracy\n",
        "    print(f\"   Validation Perplexity: {final_metrics['val_perplexity']:.2f}\")  # Exponential of loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JoR87Aeriei",
        "outputId": "31c2d3c8-b27a-414c-91da-7a55b99f5ef3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Device: CUDA\n",
            "GPU: Tesla T4\n",
            "Memory: 15.8 GB\n",
            "Set all seeds to 42\n",
            "\n",
            "📋 Model Configuration:\n",
            "   Architecture: 384d, 6L, 8H, 1536ff\n",
            "   Training: 2000 steps, batch size 24\n",
            "   Data: 500,000 tokens, seq_len 512\n",
            "Loading cached data from data_cache/tokenized_data_2000_500000.pkl\n",
            "Loaded 2000 documents and 500,000 tokens from cache\n",
            "📊 Dataset: 449540 train, 49948 val samples\n",
            "\n",
            "🚀 Training Small model with Muon optimizer\n",
            "Set all seeds to 42\n",
            "  📊 Total parameters: 32,150,976\n",
            "  Muon parameters: 13,271,040\n",
            "  AdamW parameters: 18,879,936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training:   0%|          | 0/2000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   0%|          | 0/2000 [39:50<?, ?it/s]\n",
            "W1020 14:41:34.085000 1316 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "\n",
            "\n",
            "Training:   0%|          | 10/2000 [00:06<22:25,  1.48it/s, loss=10.8048, acc=0.015, ppl=49254.8, lr=0.00e+00]\u001b[A\u001b[A\n",
            "\n",
            "Training:   0%|          | 10/2000 [00:07<22:25,  1.48it/s, loss=10.8061, acc=0.014, ppl=49318.8, lr=2.00e-04]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|          | 20/2000 [00:10<16:25,  2.01it/s, loss=10.8061, acc=0.014, ppl=49318.8, lr=2.00e-04]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|          | 20/2000 [00:10<16:25,  2.01it/s, loss=10.7881, acc=0.016, ppl=48441.5, lr=5.00e-04]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 30/2000 [00:13<13:59,  2.35it/s, loss=10.7881, acc=0.016, ppl=48441.5, lr=5.00e-04]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 30/2000 [00:14<13:59,  2.35it/s, loss=10.7615, acc=0.016, ppl=47169.0, lr=7.00e-04]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 40/2000 [00:17<13:17,  2.46it/s, loss=10.7615, acc=0.016, ppl=47169.0, lr=7.00e-04]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 40/2000 [00:18<13:17,  2.46it/s, loss=10.7341, acc=0.016, ppl=45896.3, lr=1.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▎         | 50/2000 [00:21<12:31,  2.60it/s, loss=10.7341, acc=0.016, ppl=45896.3, lr=1.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▎         | 50/2000 [00:21<12:31,  2.60it/s, loss=10.6961, acc=0.016, ppl=44181.8, lr=1.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 60/2000 [00:24<12:25,  2.60it/s, loss=10.6961, acc=0.016, ppl=44181.8, lr=1.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 60/2000 [00:25<12:25,  2.60it/s, loss=10.6617, acc=0.015, ppl=42689.0, lr=1.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▎         | 70/2000 [00:28<11:59,  2.68it/s, loss=10.6617, acc=0.015, ppl=42689.0, lr=1.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▎         | 70/2000 [00:28<11:59,  2.68it/s, loss=10.5941, acc=0.015, ppl=39899.5, lr=1.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 80/2000 [00:32<12:05,  2.65it/s, loss=10.5941, acc=0.015, ppl=39899.5, lr=1.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 80/2000 [00:32<12:05,  2.65it/s, loss=10.5332, acc=0.015, ppl=37542.7, lr=2.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 90/2000 [00:35<11:46,  2.70it/s, loss=10.5332, acc=0.015, ppl=37542.7, lr=2.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 90/2000 [00:36<11:46,  2.70it/s, loss=10.4465, acc=0.015, ppl=34424.3, lr=2.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▌         | 100/2000 [00:39<11:53,  2.66it/s, loss=10.4465, acc=0.015, ppl=34424.3, lr=2.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▌         | 100/2000 [00:40<11:53,  2.66it/s, loss=10.3463, acc=0.014, ppl=31141.7, lr=2.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 110/2000 [00:43<11:35,  2.72it/s, loss=10.3463, acc=0.014, ppl=31141.7, lr=2.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 110/2000 [00:43<11:35,  2.72it/s, loss=10.2591, acc=0.015, ppl=28540.5, lr=2.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 120/2000 [00:47<11:39,  2.69it/s, loss=10.2591, acc=0.015, ppl=28540.5, lr=2.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 120/2000 [00:47<11:39,  2.69it/s, loss=10.0906, acc=0.015, ppl=24115.2, lr=3.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▋         | 130/2000 [00:50<11:20,  2.75it/s, loss=10.0906, acc=0.015, ppl=24115.2, lr=3.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▋         | 130/2000 [00:51<11:20,  2.75it/s, loss=9.9858, acc=0.017, ppl=21715.7, lr=3.20e-03] \u001b[A\u001b[A\n",
            "\n",
            "Training:   7%|▋         | 140/2000 [00:54<11:24,  2.72it/s, loss=9.9858, acc=0.017, ppl=21715.7, lr=3.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   7%|▋         | 140/2000 [00:54<11:24,  2.72it/s, loss=9.8296, acc=0.022, ppl=18575.0, lr=3.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 150/2000 [00:57<11:05,  2.78it/s, loss=9.8296, acc=0.022, ppl=18575.0, lr=3.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 150/2000 [00:58<11:05,  2.78it/s, loss=9.6764, acc=0.032, ppl=15936.7, lr=3.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 160/2000 [01:01<11:09,  2.75it/s, loss=9.6764, acc=0.032, ppl=15936.7, lr=3.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 160/2000 [01:01<11:09,  2.75it/s, loss=9.5403, acc=0.050, ppl=13909.4, lr=4.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 170/2000 [01:04<10:52,  2.80it/s, loss=9.5403, acc=0.050, ppl=13909.4, lr=4.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 170/2000 [01:05<10:52,  2.80it/s, loss=9.3655, acc=0.090, ppl=11678.2, lr=4.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▉         | 180/2000 [01:08<10:58,  2.77it/s, loss=9.3655, acc=0.090, ppl=11678.2, lr=4.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▉         | 180/2000 [01:08<10:58,  2.77it/s, loss=9.2029, acc=0.091, ppl=9925.8, lr=4.50e-03] \u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|▉         | 190/2000 [01:11<10:41,  2.82it/s, loss=9.2029, acc=0.091, ppl=9925.8, lr=4.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|▉         | 190/2000 [01:12<10:41,  2.82it/s, loss=9.0627, acc=0.092, ppl=8627.8, lr=4.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|█         | 200/2000 [01:15<10:47,  2.78it/s, loss=9.0627, acc=0.092, ppl=8627.8, lr=4.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|█         | 200/2000 [01:16<10:47,  2.78it/s, loss=8.8473, acc=0.084, ppl=6955.8, lr=5.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|█         | 210/2000 [01:19<10:31,  2.84it/s, loss=8.8473, acc=0.084, ppl=6955.8, lr=5.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|█         | 210/2000 [01:19<10:31,  2.84it/s, loss=8.6532, acc=0.095, ppl=5728.3, lr=5.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  11%|█         | 220/2000 [01:22<10:37,  2.79it/s, loss=8.6532, acc=0.095, ppl=5728.3, lr=5.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  11%|█         | 220/2000 [01:23<10:37,  2.79it/s, loss=8.4298, acc=0.093, ppl=4581.6, lr=5.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 230/2000 [01:26<10:22,  2.84it/s, loss=8.4298, acc=0.093, ppl=4581.6, lr=5.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 230/2000 [01:26<10:22,  2.84it/s, loss=8.3305, acc=0.089, ppl=4148.5, lr=5.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 240/2000 [01:29<10:28,  2.80it/s, loss=8.3305, acc=0.089, ppl=4148.5, lr=5.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 240/2000 [01:30<10:28,  2.80it/s, loss=8.1046, acc=0.106, ppl=3309.5, lr=6.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▎        | 250/2000 [01:33<10:14,  2.85it/s, loss=8.1046, acc=0.106, ppl=3309.5, lr=6.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▎        | 250/2000 [01:33<10:14,  2.85it/s, loss=8.0646, acc=0.115, ppl=3179.8, lr=6.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 260/2000 [01:36<10:21,  2.80it/s, loss=8.0646, acc=0.115, ppl=3179.8, lr=6.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 260/2000 [01:37<10:21,  2.80it/s, loss=7.8730, acc=0.123, ppl=2625.5, lr=6.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▎        | 270/2000 [01:40<10:07,  2.85it/s, loss=7.8730, acc=0.123, ppl=2625.5, lr=6.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▎        | 270/2000 [01:40<10:07,  2.85it/s, loss=7.7415, acc=0.133, ppl=2302.0, lr=6.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 280/2000 [01:43<10:15,  2.80it/s, loss=7.7415, acc=0.133, ppl=2302.0, lr=6.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 280/2000 [01:44<10:15,  2.80it/s, loss=7.7663, acc=0.129, ppl=2359.6, lr=7.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 290/2000 [01:47<10:02,  2.84it/s, loss=7.7663, acc=0.129, ppl=2359.6, lr=7.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 290/2000 [01:47<10:02,  2.84it/s, loss=7.5427, acc=0.132, ppl=1886.8, lr=7.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▌        | 300/2000 [01:51<10:10,  2.79it/s, loss=7.5427, acc=0.132, ppl=1886.8, lr=7.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▌        | 300/2000 [01:51<10:10,  2.79it/s, loss=7.2758, acc=0.146, ppl=1444.9, lr=7.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 310/2000 [01:54<09:57,  2.83it/s, loss=7.2758, acc=0.146, ppl=1444.9, lr=7.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 310/2000 [01:55<09:57,  2.83it/s, loss=7.3134, acc=0.150, ppl=1500.3, lr=7.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 320/2000 [01:58<10:05,  2.77it/s, loss=7.3134, acc=0.150, ppl=1500.3, lr=7.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 320/2000 [01:58<10:05,  2.77it/s, loss=7.3200, acc=0.151, ppl=1510.2, lr=8.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▋        | 330/2000 [02:01<09:52,  2.82it/s, loss=7.3200, acc=0.151, ppl=1510.2, lr=8.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▋        | 330/2000 [02:02<09:52,  2.82it/s, loss=7.0291, acc=0.155, ppl=1129.0, lr=8.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  17%|█▋        | 340/2000 [02:05<09:59,  2.77it/s, loss=7.0291, acc=0.155, ppl=1129.0, lr=8.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  17%|█▋        | 340/2000 [02:05<09:59,  2.77it/s, loss=6.9560, acc=0.163, ppl=1049.5, lr=8.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 350/2000 [02:08<09:47,  2.81it/s, loss=6.9560, acc=0.163, ppl=1049.5, lr=8.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 350/2000 [02:09<09:47,  2.81it/s, loss=6.8798, acc=0.167, ppl=972.4, lr=8.70e-03] \u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 360/2000 [02:12<09:54,  2.76it/s, loss=6.8798, acc=0.167, ppl=972.4, lr=8.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 360/2000 [02:13<09:54,  2.76it/s, loss=6.8936, acc=0.173, ppl=986.0, lr=9.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 370/2000 [02:16<09:40,  2.81it/s, loss=6.8936, acc=0.173, ppl=986.0, lr=9.00e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 370/2000 [02:16<09:40,  2.81it/s, loss=6.6649, acc=0.174, ppl=784.4, lr=9.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▉        | 380/2000 [02:19<09:46,  2.76it/s, loss=6.6649, acc=0.174, ppl=784.4, lr=9.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▉        | 380/2000 [02:20<09:46,  2.76it/s, loss=6.5067, acc=0.180, ppl=669.6, lr=9.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|█▉        | 390/2000 [02:23<09:33,  2.81it/s, loss=6.5067, acc=0.180, ppl=669.6, lr=9.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|█▉        | 390/2000 [02:23<09:33,  2.81it/s, loss=6.5553, acc=0.185, ppl=703.0, lr=9.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|██        | 400/2000 [02:27<09:39,  2.76it/s, loss=6.5553, acc=0.185, ppl=703.0, lr=9.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|██        | 400/2000 [02:27<09:39,  2.76it/s, loss=6.5378, acc=0.182, ppl=690.8, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|██        | 410/2000 [02:30<09:25,  2.81it/s, loss=6.5378, acc=0.182, ppl=690.8, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|██        | 410/2000 [02:30<09:25,  2.81it/s, loss=6.4857, acc=0.187, ppl=655.7, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██        | 420/2000 [02:34<09:30,  2.77it/s, loss=6.4857, acc=0.187, ppl=655.7, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██        | 420/2000 [02:34<09:30,  2.77it/s, loss=6.4945, acc=0.184, ppl=661.5, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 430/2000 [02:37<09:17,  2.82it/s, loss=6.4945, acc=0.184, ppl=661.5, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 430/2000 [02:38<09:17,  2.82it/s, loss=6.4462, acc=0.188, ppl=630.3, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 440/2000 [02:41<09:22,  2.77it/s, loss=6.4462, acc=0.188, ppl=630.3, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 440/2000 [02:41<09:22,  2.77it/s, loss=6.3100, acc=0.188, ppl=550.0, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▎       | 450/2000 [02:44<09:09,  2.82it/s, loss=6.3100, acc=0.188, ppl=550.0, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▎       | 450/2000 [02:45<09:09,  2.82it/s, loss=6.2372, acc=0.195, ppl=511.4, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 460/2000 [02:48<09:14,  2.78it/s, loss=6.2372, acc=0.195, ppl=511.4, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 460/2000 [02:48<09:14,  2.78it/s, loss=6.2825, acc=0.191, ppl=535.1, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▎       | 470/2000 [02:51<09:00,  2.83it/s, loss=6.2825, acc=0.191, ppl=535.1, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▎       | 470/2000 [02:52<09:00,  2.83it/s, loss=6.0477, acc=0.204, ppl=423.1, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 480/2000 [02:55<09:05,  2.78it/s, loss=6.0477, acc=0.204, ppl=423.1, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 480/2000 [02:55<09:05,  2.78it/s, loss=6.0087, acc=0.205, ppl=406.9, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 490/2000 [02:58<08:52,  2.83it/s, loss=6.0087, acc=0.205, ppl=406.9, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 490/2000 [02:59<08:52,  2.83it/s, loss=5.9969, acc=0.216, ppl=402.2, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▌       | 500/2000 [03:02<08:58,  2.79it/s, loss=5.9969, acc=0.216, ppl=402.2, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▌       | 500/2000 [03:03<08:58,  2.79it/s, loss=6.1523, acc=0.196, ppl=469.8, lr=1.00e-02]\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 500: Val Loss: 5.9331, Val Acc: 0.2099, Val PPL: 377.32\n",
            "💾 Saved best model with val_loss: 5.9331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training:  26%|██▌       | 510/2000 [03:18<18:06,  1.37it/s, loss=6.1523, acc=0.196, ppl=469.8, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 510/2000 [03:19<18:06,  1.37it/s, loss=6.0776, acc=0.194, ppl=436.0, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 520/2000 [03:22<15:21,  1.61it/s, loss=6.0776, acc=0.194, ppl=436.0, lr=1.00e-02]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 520/2000 [03:22<15:21,  1.61it/s, loss=5.8630, acc=0.209, ppl=351.8, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▋       | 530/2000 [03:25<13:10,  1.86it/s, loss=5.8630, acc=0.209, ppl=351.8, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▋       | 530/2000 [03:26<13:10,  1.86it/s, loss=5.9263, acc=0.210, ppl=374.7, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 540/2000 [03:29<11:53,  2.05it/s, loss=5.9263, acc=0.210, ppl=374.7, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 540/2000 [03:29<11:53,  2.05it/s, loss=5.8789, acc=0.218, ppl=357.4, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 550/2000 [03:32<10:44,  2.25it/s, loss=5.8789, acc=0.218, ppl=357.4, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 550/2000 [03:33<10:44,  2.25it/s, loss=5.8223, acc=0.209, ppl=337.8, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 560/2000 [03:36<10:09,  2.36it/s, loss=5.8223, acc=0.209, ppl=337.8, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 560/2000 [03:37<10:09,  2.36it/s, loss=5.7772, acc=0.211, ppl=322.9, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 570/2000 [03:40<09:29,  2.51it/s, loss=5.7772, acc=0.211, ppl=322.9, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 570/2000 [03:40<09:29,  2.51it/s, loss=5.7774, acc=0.203, ppl=322.9, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▉       | 580/2000 [03:43<09:15,  2.56it/s, loss=5.7774, acc=0.203, ppl=322.9, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▉       | 580/2000 [03:44<09:15,  2.56it/s, loss=5.7546, acc=0.210, ppl=315.6, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|██▉       | 590/2000 [03:47<08:49,  2.66it/s, loss=5.7546, acc=0.210, ppl=315.6, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|██▉       | 590/2000 [03:47<08:49,  2.66it/s, loss=5.8642, acc=0.210, ppl=352.2, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|███       | 600/2000 [03:50<08:45,  2.66it/s, loss=5.8642, acc=0.210, ppl=352.2, lr=9.99e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|███       | 600/2000 [03:51<08:45,  2.66it/s, loss=5.7278, acc=0.225, ppl=307.3, lr=9.98e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|███       | 610/2000 [03:54<08:27,  2.74it/s, loss=5.7278, acc=0.225, ppl=307.3, lr=9.98e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|███       | 610/2000 [03:54<08:27,  2.74it/s, loss=5.6763, acc=0.224, ppl=291.9, lr=9.98e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███       | 620/2000 [03:58<08:27,  2.72it/s, loss=5.6763, acc=0.224, ppl=291.9, lr=9.98e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███       | 620/2000 [03:58<08:27,  2.72it/s, loss=5.7520, acc=0.220, ppl=314.8, lr=9.98e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 630/2000 [04:01<08:12,  2.78it/s, loss=5.7520, acc=0.220, ppl=314.8, lr=9.98e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 630/2000 [04:01<08:12,  2.78it/s, loss=5.7667, acc=0.215, ppl=319.5, lr=9.98e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 640/2000 [04:05<08:14,  2.75it/s, loss=5.7667, acc=0.215, ppl=319.5, lr=9.98e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 640/2000 [04:05<08:14,  2.75it/s, loss=5.6927, acc=0.217, ppl=296.7, lr=9.98e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▎      | 650/2000 [04:08<08:01,  2.80it/s, loss=5.6927, acc=0.217, ppl=296.7, lr=9.98e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▎      | 650/2000 [04:09<08:01,  2.80it/s, loss=5.6873, acc=0.212, ppl=295.1, lr=9.98e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  33%|███▎      | 660/2000 [04:12<08:04,  2.76it/s, loss=5.6873, acc=0.212, ppl=295.1, lr=9.98e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  33%|███▎      | 660/2000 [04:12<08:04,  2.76it/s, loss=5.6904, acc=0.221, ppl=296.0, lr=9.97e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▎      | 670/2000 [04:15<07:52,  2.82it/s, loss=5.6904, acc=0.221, ppl=296.0, lr=9.97e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▎      | 670/2000 [04:16<07:52,  2.82it/s, loss=5.5522, acc=0.234, ppl=257.8, lr=9.97e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 680/2000 [04:19<07:56,  2.77it/s, loss=5.5522, acc=0.234, ppl=257.8, lr=9.97e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 680/2000 [04:19<07:56,  2.77it/s, loss=5.4287, acc=0.235, ppl=227.8, lr=9.97e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 690/2000 [04:22<07:44,  2.82it/s, loss=5.4287, acc=0.235, ppl=227.8, lr=9.97e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 690/2000 [04:23<07:44,  2.82it/s, loss=5.6103, acc=0.222, ppl=273.2, lr=9.97e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▌      | 700/2000 [04:26<07:47,  2.78it/s, loss=5.6103, acc=0.222, ppl=273.2, lr=9.97e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▌      | 700/2000 [04:26<07:47,  2.78it/s, loss=5.4861, acc=0.233, ppl=241.3, lr=9.97e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 710/2000 [04:30<07:35,  2.83it/s, loss=5.4861, acc=0.233, ppl=241.3, lr=9.97e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 710/2000 [04:30<07:35,  2.83it/s, loss=5.5245, acc=0.240, ppl=250.8, lr=9.96e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 720/2000 [04:33<07:39,  2.78it/s, loss=5.5245, acc=0.240, ppl=250.8, lr=9.96e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 720/2000 [04:34<07:39,  2.78it/s, loss=5.3258, acc=0.245, ppl=205.6, lr=9.96e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▋      | 730/2000 [04:37<07:28,  2.83it/s, loss=5.3258, acc=0.245, ppl=205.6, lr=9.96e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▋      | 730/2000 [04:37<07:28,  2.83it/s, loss=5.5662, acc=0.226, ppl=261.4, lr=9.96e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 740/2000 [04:40<07:32,  2.79it/s, loss=5.5662, acc=0.226, ppl=261.4, lr=9.96e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 740/2000 [04:41<07:32,  2.79it/s, loss=5.3228, acc=0.240, ppl=205.0, lr=9.96e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 750/2000 [04:44<07:21,  2.83it/s, loss=5.3228, acc=0.240, ppl=205.0, lr=9.96e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 750/2000 [04:44<07:21,  2.83it/s, loss=5.3365, acc=0.250, ppl=207.8, lr=9.95e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 760/2000 [04:47<07:25,  2.79it/s, loss=5.3365, acc=0.250, ppl=207.8, lr=9.95e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 760/2000 [04:48<07:25,  2.79it/s, loss=5.3464, acc=0.237, ppl=209.9, lr=9.95e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 770/2000 [04:51<07:14,  2.83it/s, loss=5.3464, acc=0.237, ppl=209.9, lr=9.95e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 770/2000 [04:51<07:14,  2.83it/s, loss=5.3160, acc=0.249, ppl=203.6, lr=9.95e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▉      | 780/2000 [04:55<07:17,  2.79it/s, loss=5.3160, acc=0.249, ppl=203.6, lr=9.95e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▉      | 780/2000 [04:55<07:17,  2.79it/s, loss=5.2146, acc=0.244, ppl=183.9, lr=9.94e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|███▉      | 790/2000 [04:58<07:06,  2.83it/s, loss=5.2146, acc=0.244, ppl=183.9, lr=9.94e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|███▉      | 790/2000 [04:58<07:06,  2.83it/s, loss=5.3025, acc=0.254, ppl=200.8, lr=9.94e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|████      | 800/2000 [05:02<07:10,  2.79it/s, loss=5.3025, acc=0.254, ppl=200.8, lr=9.94e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|████      | 800/2000 [05:02<07:10,  2.79it/s, loss=5.3924, acc=0.239, ppl=219.7, lr=9.94e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|████      | 810/2000 [05:05<06:59,  2.83it/s, loss=5.3924, acc=0.239, ppl=219.7, lr=9.94e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|████      | 810/2000 [05:06<06:59,  2.83it/s, loss=5.2813, acc=0.252, ppl=196.6, lr=9.94e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████      | 820/2000 [05:09<07:03,  2.78it/s, loss=5.2813, acc=0.252, ppl=196.6, lr=9.94e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████      | 820/2000 [05:09<07:03,  2.78it/s, loss=5.1781, acc=0.265, ppl=177.3, lr=9.93e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 830/2000 [05:12<06:53,  2.83it/s, loss=5.1781, acc=0.265, ppl=177.3, lr=9.93e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 830/2000 [05:13<06:53,  2.83it/s, loss=5.3016, acc=0.260, ppl=200.7, lr=9.93e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 840/2000 [05:16<06:56,  2.78it/s, loss=5.3016, acc=0.260, ppl=200.7, lr=9.93e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 840/2000 [05:16<06:56,  2.78it/s, loss=5.1200, acc=0.268, ppl=167.3, lr=9.93e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▎     | 850/2000 [05:19<06:46,  2.83it/s, loss=5.1200, acc=0.268, ppl=167.3, lr=9.93e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▎     | 850/2000 [05:20<06:46,  2.83it/s, loss=5.1804, acc=0.261, ppl=177.8, lr=9.92e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 860/2000 [05:23<06:49,  2.78it/s, loss=5.1804, acc=0.261, ppl=177.8, lr=9.92e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 860/2000 [05:23<06:49,  2.78it/s, loss=5.0913, acc=0.261, ppl=162.6, lr=9.92e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▎     | 870/2000 [05:27<06:39,  2.83it/s, loss=5.0913, acc=0.261, ppl=162.6, lr=9.92e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▎     | 870/2000 [05:27<06:39,  2.83it/s, loss=5.1475, acc=0.255, ppl=172.0, lr=9.92e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▍     | 880/2000 [05:30<06:42,  2.78it/s, loss=5.1475, acc=0.255, ppl=172.0, lr=9.92e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▍     | 880/2000 [05:31<06:42,  2.78it/s, loss=5.0186, acc=0.292, ppl=151.2, lr=9.91e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▍     | 890/2000 [05:34<06:32,  2.83it/s, loss=5.0186, acc=0.292, ppl=151.2, lr=9.91e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▍     | 890/2000 [05:34<06:32,  2.83it/s, loss=5.0278, acc=0.272, ppl=152.6, lr=9.91e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▌     | 900/2000 [05:37<06:35,  2.78it/s, loss=5.0278, acc=0.272, ppl=152.6, lr=9.91e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▌     | 900/2000 [05:38<06:35,  2.78it/s, loss=4.8695, acc=0.278, ppl=130.3, lr=9.90e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 910/2000 [05:41<06:25,  2.83it/s, loss=4.8695, acc=0.278, ppl=130.3, lr=9.90e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 910/2000 [05:41<06:25,  2.83it/s, loss=5.1841, acc=0.265, ppl=178.4, lr=9.90e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 920/2000 [05:45<06:28,  2.78it/s, loss=5.1841, acc=0.265, ppl=178.4, lr=9.90e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 920/2000 [05:45<06:28,  2.78it/s, loss=4.9229, acc=0.283, ppl=137.4, lr=9.90e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▋     | 930/2000 [05:48<06:18,  2.83it/s, loss=4.9229, acc=0.283, ppl=137.4, lr=9.90e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▋     | 930/2000 [05:48<06:18,  2.83it/s, loss=5.0310, acc=0.262, ppl=153.1, lr=9.89e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 940/2000 [05:52<06:21,  2.78it/s, loss=5.0310, acc=0.262, ppl=153.1, lr=9.89e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 940/2000 [05:52<06:21,  2.78it/s, loss=5.0905, acc=0.270, ppl=162.5, lr=9.89e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 950/2000 [05:55<06:11,  2.83it/s, loss=5.0905, acc=0.270, ppl=162.5, lr=9.89e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 950/2000 [05:56<06:11,  2.83it/s, loss=5.0762, acc=0.276, ppl=160.2, lr=9.89e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 960/2000 [05:59<06:14,  2.78it/s, loss=5.0762, acc=0.276, ppl=160.2, lr=9.89e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 960/2000 [05:59<06:14,  2.78it/s, loss=4.9757, acc=0.285, ppl=144.8, lr=9.88e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 970/2000 [06:02<06:04,  2.83it/s, loss=4.9757, acc=0.285, ppl=144.8, lr=9.88e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 970/2000 [06:03<06:04,  2.83it/s, loss=4.7840, acc=0.295, ppl=119.6, lr=9.88e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▉     | 980/2000 [06:06<06:06,  2.78it/s, loss=4.7840, acc=0.295, ppl=119.6, lr=9.88e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▉     | 980/2000 [06:06<06:06,  2.78it/s, loss=4.9623, acc=0.289, ppl=142.9, lr=9.87e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|████▉     | 990/2000 [06:09<05:57,  2.83it/s, loss=4.9623, acc=0.289, ppl=142.9, lr=9.87e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|████▉     | 990/2000 [06:10<05:57,  2.83it/s, loss=4.7667, acc=0.297, ppl=117.5, lr=9.87e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|█████     | 1000/2000 [06:13<05:59,  2.78it/s, loss=4.7667, acc=0.297, ppl=117.5, lr=9.87e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|█████     | 1000/2000 [06:13<05:59,  2.78it/s, loss=4.9164, acc=0.287, ppl=136.5, lr=9.86e-03]\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1000: Val Loss: 4.6150, Val Acc: 0.3280, Val PPL: 100.99\n",
            "💾 Saved best model with val_loss: 4.6150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training:  50%|█████     | 1010/2000 [06:29<12:16,  1.34it/s, loss=4.9164, acc=0.287, ppl=136.5, lr=9.86e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|█████     | 1010/2000 [06:30<12:16,  1.34it/s, loss=4.8647, acc=0.308, ppl=129.6, lr=9.86e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████     | 1020/2000 [06:33<10:20,  1.58it/s, loss=4.8647, acc=0.308, ppl=129.6, lr=9.86e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████     | 1020/2000 [06:34<10:20,  1.58it/s, loss=4.7778, acc=0.311, ppl=118.8, lr=9.85e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 1030/2000 [06:37<08:48,  1.84it/s, loss=4.7778, acc=0.311, ppl=118.8, lr=9.85e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 1030/2000 [06:37<08:48,  1.84it/s, loss=4.8822, acc=0.292, ppl=131.9, lr=9.85e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 1040/2000 [06:40<07:53,  2.03it/s, loss=4.8822, acc=0.292, ppl=131.9, lr=9.85e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 1040/2000 [06:41<07:53,  2.03it/s, loss=4.9360, acc=0.294, ppl=139.2, lr=9.84e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▎    | 1050/2000 [06:44<07:04,  2.24it/s, loss=4.9360, acc=0.294, ppl=139.2, lr=9.84e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▎    | 1050/2000 [06:44<07:04,  2.24it/s, loss=4.7516, acc=0.305, ppl=115.8, lr=9.84e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 1060/2000 [06:47<06:39,  2.35it/s, loss=4.7516, acc=0.305, ppl=115.8, lr=9.84e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 1060/2000 [06:48<06:39,  2.35it/s, loss=4.8651, acc=0.299, ppl=129.7, lr=9.83e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▎    | 1070/2000 [06:51<06:11,  2.50it/s, loss=4.8651, acc=0.299, ppl=129.7, lr=9.83e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▎    | 1070/2000 [06:51<06:11,  2.50it/s, loss=4.8022, acc=0.298, ppl=121.8, lr=9.83e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▍    | 1080/2000 [06:55<06:00,  2.55it/s, loss=4.8022, acc=0.298, ppl=121.8, lr=9.83e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▍    | 1080/2000 [06:55<06:00,  2.55it/s, loss=4.7761, acc=0.296, ppl=118.6, lr=9.82e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▍    | 1090/2000 [06:58<05:41,  2.66it/s, loss=4.7761, acc=0.296, ppl=118.6, lr=9.82e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▍    | 1090/2000 [06:58<05:41,  2.66it/s, loss=4.7371, acc=0.310, ppl=114.1, lr=9.82e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▌    | 1100/2000 [07:02<05:37,  2.67it/s, loss=4.7371, acc=0.310, ppl=114.1, lr=9.82e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▌    | 1100/2000 [07:02<05:37,  2.67it/s, loss=4.5168, acc=0.331, ppl=91.5, lr=9.81e-03] \u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▌    | 1110/2000 [07:05<05:24,  2.74it/s, loss=4.5168, acc=0.331, ppl=91.5, lr=9.81e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▌    | 1110/2000 [07:06<05:24,  2.74it/s, loss=4.5302, acc=0.316, ppl=92.8, lr=9.81e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▌    | 1120/2000 [07:09<05:23,  2.72it/s, loss=4.5302, acc=0.316, ppl=92.8, lr=9.81e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▌    | 1120/2000 [07:09<05:23,  2.72it/s, loss=4.7369, acc=0.294, ppl=114.1, lr=9.80e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▋    | 1130/2000 [07:12<05:12,  2.79it/s, loss=4.7369, acc=0.294, ppl=114.1, lr=9.80e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▋    | 1130/2000 [07:13<05:12,  2.79it/s, loss=4.7863, acc=0.317, ppl=119.9, lr=9.80e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 1140/2000 [07:16<05:12,  2.75it/s, loss=4.7863, acc=0.317, ppl=119.9, lr=9.80e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 1140/2000 [07:16<05:12,  2.75it/s, loss=4.6217, acc=0.316, ppl=101.7, lr=9.79e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▊    | 1150/2000 [07:19<05:02,  2.81it/s, loss=4.6217, acc=0.316, ppl=101.7, lr=9.79e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▊    | 1150/2000 [07:20<05:02,  2.81it/s, loss=4.6684, acc=0.294, ppl=106.5, lr=9.79e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 1160/2000 [07:23<05:03,  2.77it/s, loss=4.6684, acc=0.294, ppl=106.5, lr=9.79e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 1160/2000 [07:23<05:03,  2.77it/s, loss=4.5521, acc=0.335, ppl=94.8, lr=9.78e-03] \u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 1170/2000 [07:26<04:54,  2.81it/s, loss=4.5521, acc=0.335, ppl=94.8, lr=9.78e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 1170/2000 [07:27<04:54,  2.81it/s, loss=4.5365, acc=0.327, ppl=93.4, lr=9.78e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▉    | 1180/2000 [07:30<04:56,  2.77it/s, loss=4.5365, acc=0.327, ppl=93.4, lr=9.78e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▉    | 1180/2000 [07:31<04:56,  2.77it/s, loss=4.5341, acc=0.332, ppl=93.1, lr=9.77e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|█████▉    | 1190/2000 [07:34<04:47,  2.82it/s, loss=4.5341, acc=0.332, ppl=93.1, lr=9.77e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|█████▉    | 1190/2000 [07:34<04:47,  2.82it/s, loss=4.5994, acc=0.321, ppl=99.4, lr=9.76e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|██████    | 1200/2000 [07:37<04:48,  2.77it/s, loss=4.5994, acc=0.321, ppl=99.4, lr=9.76e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|██████    | 1200/2000 [07:38<04:48,  2.77it/s, loss=4.5609, acc=0.327, ppl=95.7, lr=9.76e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|██████    | 1210/2000 [07:41<04:40,  2.82it/s, loss=4.5609, acc=0.327, ppl=95.7, lr=9.76e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|██████    | 1210/2000 [07:41<04:40,  2.82it/s, loss=4.4673, acc=0.340, ppl=87.1, lr=9.75e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████    | 1220/2000 [07:45<04:41,  2.77it/s, loss=4.4673, acc=0.340, ppl=87.1, lr=9.75e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████    | 1220/2000 [07:45<04:41,  2.77it/s, loss=4.5859, acc=0.333, ppl=98.1, lr=9.74e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 1230/2000 [07:48<04:32,  2.82it/s, loss=4.5859, acc=0.333, ppl=98.1, lr=9.74e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 1230/2000 [07:48<04:32,  2.82it/s, loss=4.7357, acc=0.314, ppl=113.9, lr=9.74e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 1240/2000 [07:52<04:33,  2.78it/s, loss=4.7357, acc=0.314, ppl=113.9, lr=9.74e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 1240/2000 [07:52<04:33,  2.78it/s, loss=4.6445, acc=0.311, ppl=104.0, lr=9.73e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▎   | 1250/2000 [07:55<04:25,  2.83it/s, loss=4.6445, acc=0.311, ppl=104.0, lr=9.73e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▎   | 1250/2000 [07:56<04:25,  2.83it/s, loss=4.5352, acc=0.321, ppl=93.2, lr=9.73e-03] \u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 1260/2000 [07:59<04:25,  2.78it/s, loss=4.5352, acc=0.321, ppl=93.2, lr=9.73e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 1260/2000 [07:59<04:25,  2.78it/s, loss=4.5838, acc=0.339, ppl=97.9, lr=9.72e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▎   | 1270/2000 [08:02<04:17,  2.83it/s, loss=4.5838, acc=0.339, ppl=97.9, lr=9.72e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▎   | 1270/2000 [08:03<04:17,  2.83it/s, loss=4.4482, acc=0.345, ppl=85.5, lr=9.71e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 1280/2000 [08:06<04:18,  2.78it/s, loss=4.4482, acc=0.345, ppl=85.5, lr=9.71e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 1280/2000 [08:06<04:18,  2.78it/s, loss=4.5207, acc=0.333, ppl=91.9, lr=9.71e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 1290/2000 [08:09<04:10,  2.83it/s, loss=4.5207, acc=0.333, ppl=91.9, lr=9.71e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 1290/2000 [08:10<04:10,  2.83it/s, loss=4.5348, acc=0.334, ppl=93.2, lr=9.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▌   | 1300/2000 [08:13<04:11,  2.78it/s, loss=4.5348, acc=0.334, ppl=93.2, lr=9.70e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▌   | 1300/2000 [08:13<04:11,  2.78it/s, loss=4.3964, acc=0.340, ppl=81.2, lr=9.69e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 1310/2000 [08:16<04:03,  2.83it/s, loss=4.3964, acc=0.340, ppl=81.2, lr=9.69e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 1310/2000 [08:17<04:03,  2.83it/s, loss=4.3992, acc=0.344, ppl=81.4, lr=9.69e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 1320/2000 [08:20<04:04,  2.78it/s, loss=4.3992, acc=0.344, ppl=81.4, lr=9.69e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 1320/2000 [08:20<04:04,  2.78it/s, loss=4.4413, acc=0.342, ppl=84.9, lr=9.68e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▋   | 1330/2000 [08:24<03:56,  2.83it/s, loss=4.4413, acc=0.342, ppl=84.9, lr=9.68e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▋   | 1330/2000 [08:24<03:56,  2.83it/s, loss=4.3993, acc=0.348, ppl=81.4, lr=9.67e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  67%|██████▋   | 1340/2000 [08:27<03:56,  2.78it/s, loss=4.3993, acc=0.348, ppl=81.4, lr=9.67e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  67%|██████▋   | 1340/2000 [08:28<03:56,  2.78it/s, loss=4.4667, acc=0.343, ppl=87.1, lr=9.66e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 1350/2000 [08:31<03:49,  2.83it/s, loss=4.4667, acc=0.343, ppl=87.1, lr=9.66e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 1350/2000 [08:31<03:49,  2.83it/s, loss=4.3044, acc=0.355, ppl=74.0, lr=9.66e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 1360/2000 [08:34<03:49,  2.78it/s, loss=4.3044, acc=0.355, ppl=74.0, lr=9.66e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 1360/2000 [08:35<03:49,  2.78it/s, loss=4.3686, acc=0.351, ppl=78.9, lr=9.65e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 1370/2000 [08:38<03:42,  2.83it/s, loss=4.3686, acc=0.351, ppl=78.9, lr=9.65e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 1370/2000 [08:38<03:42,  2.83it/s, loss=4.4254, acc=0.343, ppl=83.5, lr=9.64e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▉   | 1380/2000 [08:41<03:42,  2.78it/s, loss=4.4254, acc=0.343, ppl=83.5, lr=9.64e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▉   | 1380/2000 [08:42<03:42,  2.78it/s, loss=4.4735, acc=0.346, ppl=87.7, lr=9.64e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|██████▉   | 1390/2000 [08:45<03:35,  2.83it/s, loss=4.4735, acc=0.346, ppl=87.7, lr=9.64e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|██████▉   | 1390/2000 [08:45<03:35,  2.83it/s, loss=4.3342, acc=0.359, ppl=76.3, lr=9.63e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|███████   | 1400/2000 [08:49<03:35,  2.78it/s, loss=4.3342, acc=0.359, ppl=76.3, lr=9.63e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|███████   | 1400/2000 [08:49<03:35,  2.78it/s, loss=4.2958, acc=0.366, ppl=73.4, lr=9.62e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|███████   | 1410/2000 [08:52<03:28,  2.83it/s, loss=4.2958, acc=0.366, ppl=73.4, lr=9.62e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|███████   | 1410/2000 [08:52<03:28,  2.83it/s, loss=4.3200, acc=0.365, ppl=75.2, lr=9.61e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████   | 1420/2000 [08:56<03:28,  2.79it/s, loss=4.3200, acc=0.365, ppl=75.2, lr=9.61e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████   | 1420/2000 [08:56<03:28,  2.79it/s, loss=4.3229, acc=0.358, ppl=75.4, lr=9.61e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 1430/2000 [08:59<03:21,  2.83it/s, loss=4.3229, acc=0.358, ppl=75.4, lr=9.61e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 1430/2000 [09:00<03:21,  2.83it/s, loss=4.2475, acc=0.371, ppl=69.9, lr=9.60e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 1440/2000 [09:03<03:21,  2.79it/s, loss=4.2475, acc=0.371, ppl=69.9, lr=9.60e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 1440/2000 [09:03<03:21,  2.79it/s, loss=4.3094, acc=0.361, ppl=74.4, lr=9.59e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▎  | 1450/2000 [09:06<03:14,  2.83it/s, loss=4.3094, acc=0.361, ppl=74.4, lr=9.59e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▎  | 1450/2000 [09:07<03:14,  2.83it/s, loss=4.3670, acc=0.359, ppl=78.8, lr=9.58e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 1460/2000 [09:10<03:13,  2.79it/s, loss=4.3670, acc=0.359, ppl=78.8, lr=9.58e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 1460/2000 [09:10<03:13,  2.79it/s, loss=4.2788, acc=0.363, ppl=72.2, lr=9.57e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▎  | 1470/2000 [09:13<03:07,  2.83it/s, loss=4.2788, acc=0.363, ppl=72.2, lr=9.57e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▎  | 1470/2000 [09:14<03:07,  2.83it/s, loss=4.2756, acc=0.350, ppl=71.9, lr=9.57e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 1480/2000 [09:17<03:06,  2.79it/s, loss=4.2756, acc=0.350, ppl=71.9, lr=9.57e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 1480/2000 [09:17<03:06,  2.79it/s, loss=4.1340, acc=0.385, ppl=62.4, lr=9.56e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 1490/2000 [09:20<03:00,  2.83it/s, loss=4.1340, acc=0.385, ppl=62.4, lr=9.56e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 1490/2000 [09:21<03:00,  2.83it/s, loss=4.2791, acc=0.362, ppl=72.2, lr=9.55e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▌  | 1500/2000 [09:24<02:59,  2.78it/s, loss=4.2791, acc=0.362, ppl=72.2, lr=9.55e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▌  | 1500/2000 [09:25<02:59,  2.78it/s, loss=4.2221, acc=0.369, ppl=68.2, lr=9.54e-03]\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1500: Val Loss: 3.7905, Val Acc: 0.4302, Val PPL: 44.28\n",
            "💾 Saved best model with val_loss: 3.7905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training:  76%|███████▌  | 1510/2000 [09:40<05:59,  1.36it/s, loss=4.2221, acc=0.369, ppl=68.2, lr=9.54e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 1510/2000 [09:41<05:59,  1.36it/s, loss=4.0710, acc=0.379, ppl=58.6, lr=9.54e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 1520/2000 [09:44<05:00,  1.60it/s, loss=4.0710, acc=0.379, ppl=58.6, lr=9.54e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 1520/2000 [09:44<05:00,  1.60it/s, loss=4.0592, acc=0.382, ppl=57.9, lr=9.53e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▋  | 1530/2000 [09:47<04:13,  1.85it/s, loss=4.0592, acc=0.382, ppl=57.9, lr=9.53e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▋  | 1530/2000 [09:48<04:13,  1.85it/s, loss=4.2892, acc=0.360, ppl=72.9, lr=9.52e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 1540/2000 [09:51<03:45,  2.04it/s, loss=4.2892, acc=0.360, ppl=72.9, lr=9.52e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 1540/2000 [09:51<03:45,  2.04it/s, loss=4.0346, acc=0.383, ppl=56.5, lr=9.51e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 1550/2000 [09:55<03:20,  2.25it/s, loss=4.0346, acc=0.383, ppl=56.5, lr=9.51e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 1550/2000 [09:55<03:20,  2.25it/s, loss=4.1887, acc=0.375, ppl=65.9, lr=9.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 1560/2000 [09:58<03:06,  2.36it/s, loss=4.1887, acc=0.375, ppl=65.9, lr=9.50e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 1560/2000 [09:59<03:06,  2.36it/s, loss=4.0887, acc=0.381, ppl=59.7, lr=9.49e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 1570/2000 [10:02<02:51,  2.51it/s, loss=4.0887, acc=0.381, ppl=59.7, lr=9.49e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 1570/2000 [10:02<02:51,  2.51it/s, loss=4.0436, acc=0.380, ppl=57.0, lr=9.49e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▉  | 1580/2000 [10:05<02:44,  2.56it/s, loss=4.0436, acc=0.380, ppl=57.0, lr=9.49e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▉  | 1580/2000 [10:06<02:44,  2.56it/s, loss=4.1434, acc=0.376, ppl=63.0, lr=9.48e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|███████▉  | 1590/2000 [10:09<02:33,  2.67it/s, loss=4.1434, acc=0.376, ppl=63.0, lr=9.48e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|███████▉  | 1590/2000 [10:09<02:33,  2.67it/s, loss=4.0260, acc=0.392, ppl=56.0, lr=9.47e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|████████  | 1600/2000 [10:13<02:29,  2.67it/s, loss=4.0260, acc=0.392, ppl=56.0, lr=9.47e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|████████  | 1600/2000 [10:13<02:29,  2.67it/s, loss=4.1651, acc=0.383, ppl=64.4, lr=9.46e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|████████  | 1610/2000 [10:16<02:21,  2.75it/s, loss=4.1651, acc=0.383, ppl=64.4, lr=9.46e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|████████  | 1610/2000 [10:16<02:21,  2.75it/s, loss=4.0692, acc=0.386, ppl=58.5, lr=9.45e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████  | 1620/2000 [10:20<02:19,  2.73it/s, loss=4.0692, acc=0.386, ppl=58.5, lr=9.45e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████  | 1620/2000 [10:20<02:19,  2.73it/s, loss=4.1570, acc=0.369, ppl=63.9, lr=9.44e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 1630/2000 [10:23<02:12,  2.79it/s, loss=4.1570, acc=0.369, ppl=63.9, lr=9.44e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 1630/2000 [10:24<02:12,  2.79it/s, loss=3.9526, acc=0.382, ppl=52.1, lr=9.43e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 1640/2000 [10:27<02:10,  2.76it/s, loss=3.9526, acc=0.382, ppl=52.1, lr=9.43e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 1640/2000 [10:27<02:10,  2.76it/s, loss=4.0633, acc=0.386, ppl=58.2, lr=9.42e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▎ | 1650/2000 [10:30<02:04,  2.81it/s, loss=4.0633, acc=0.386, ppl=58.2, lr=9.42e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▎ | 1650/2000 [10:31<02:04,  2.81it/s, loss=4.0356, acc=0.394, ppl=56.6, lr=9.41e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 1660/2000 [10:34<02:02,  2.77it/s, loss=4.0356, acc=0.394, ppl=56.6, lr=9.41e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 1660/2000 [10:34<02:02,  2.77it/s, loss=4.0212, acc=0.399, ppl=55.8, lr=9.40e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▎ | 1670/2000 [10:37<01:56,  2.82it/s, loss=4.0212, acc=0.399, ppl=55.8, lr=9.40e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▎ | 1670/2000 [10:38<01:56,  2.82it/s, loss=4.0936, acc=0.377, ppl=60.0, lr=9.40e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 1680/2000 [10:41<01:55,  2.78it/s, loss=4.0936, acc=0.377, ppl=60.0, lr=9.40e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 1680/2000 [10:41<01:55,  2.78it/s, loss=4.1559, acc=0.372, ppl=63.8, lr=9.38e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 1690/2000 [10:44<01:49,  2.83it/s, loss=4.1559, acc=0.372, ppl=63.8, lr=9.38e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 1690/2000 [10:45<01:49,  2.83it/s, loss=4.1143, acc=0.379, ppl=61.2, lr=9.38e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▌ | 1700/2000 [10:48<01:47,  2.78it/s, loss=4.1143, acc=0.379, ppl=61.2, lr=9.38e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▌ | 1700/2000 [10:48<01:47,  2.78it/s, loss=4.0023, acc=0.392, ppl=54.7, lr=9.37e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 1710/2000 [10:51<01:42,  2.83it/s, loss=4.0023, acc=0.392, ppl=54.7, lr=9.37e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 1710/2000 [10:52<01:42,  2.83it/s, loss=3.9411, acc=0.398, ppl=51.5, lr=9.36e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 1720/2000 [10:55<01:40,  2.79it/s, loss=3.9411, acc=0.398, ppl=51.5, lr=9.36e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 1720/2000 [10:56<01:40,  2.79it/s, loss=4.0884, acc=0.394, ppl=59.6, lr=9.35e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▋ | 1730/2000 [10:59<01:35,  2.83it/s, loss=4.0884, acc=0.394, ppl=59.6, lr=9.35e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▋ | 1730/2000 [10:59<01:35,  2.83it/s, loss=3.9600, acc=0.407, ppl=52.5, lr=9.34e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 1740/2000 [11:02<01:33,  2.78it/s, loss=3.9600, acc=0.407, ppl=52.5, lr=9.34e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 1740/2000 [11:03<01:33,  2.78it/s, loss=4.0586, acc=0.385, ppl=57.9, lr=9.33e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 1750/2000 [11:06<01:28,  2.83it/s, loss=4.0586, acc=0.385, ppl=57.9, lr=9.33e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 1750/2000 [11:06<01:28,  2.83it/s, loss=3.8169, acc=0.418, ppl=45.5, lr=9.32e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 1760/2000 [11:09<01:26,  2.78it/s, loss=3.8169, acc=0.418, ppl=45.5, lr=9.32e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 1760/2000 [11:10<01:26,  2.78it/s, loss=3.7907, acc=0.403, ppl=44.3, lr=9.31e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 1770/2000 [11:13<01:21,  2.83it/s, loss=3.7907, acc=0.403, ppl=44.3, lr=9.31e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 1770/2000 [11:13<01:21,  2.83it/s, loss=3.9084, acc=0.405, ppl=49.8, lr=9.30e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  89%|████████▉ | 1780/2000 [11:17<01:18,  2.79it/s, loss=3.9084, acc=0.405, ppl=49.8, lr=9.30e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  89%|████████▉ | 1780/2000 [11:17<01:18,  2.79it/s, loss=3.8565, acc=0.413, ppl=47.3, lr=9.29e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|████████▉ | 1790/2000 [11:20<01:14,  2.83it/s, loss=3.8565, acc=0.413, ppl=47.3, lr=9.29e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|████████▉ | 1790/2000 [11:20<01:14,  2.83it/s, loss=3.9419, acc=0.401, ppl=51.5, lr=9.28e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|█████████ | 1800/2000 [11:24<01:11,  2.78it/s, loss=3.9419, acc=0.401, ppl=51.5, lr=9.28e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|█████████ | 1800/2000 [11:24<01:11,  2.78it/s, loss=3.7590, acc=0.422, ppl=42.9, lr=9.27e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|█████████ | 1810/2000 [11:27<01:07,  2.83it/s, loss=3.7590, acc=0.422, ppl=42.9, lr=9.27e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|█████████ | 1810/2000 [11:28<01:07,  2.83it/s, loss=3.8830, acc=0.394, ppl=48.6, lr=9.26e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████ | 1820/2000 [11:31<01:04,  2.79it/s, loss=3.8830, acc=0.394, ppl=48.6, lr=9.26e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████ | 1820/2000 [11:31<01:04,  2.79it/s, loss=3.8468, acc=0.410, ppl=46.8, lr=9.25e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 1830/2000 [11:34<00:59,  2.83it/s, loss=3.8468, acc=0.410, ppl=46.8, lr=9.25e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 1830/2000 [11:35<00:59,  2.83it/s, loss=3.7679, acc=0.420, ppl=43.3, lr=9.24e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 1840/2000 [11:38<00:57,  2.79it/s, loss=3.7679, acc=0.420, ppl=43.3, lr=9.24e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 1840/2000 [11:38<00:57,  2.79it/s, loss=3.8626, acc=0.409, ppl=47.6, lr=9.23e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▎| 1850/2000 [11:41<00:52,  2.84it/s, loss=3.8626, acc=0.409, ppl=47.6, lr=9.23e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▎| 1850/2000 [11:42<00:52,  2.84it/s, loss=3.8742, acc=0.402, ppl=48.1, lr=9.22e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 1860/2000 [11:45<00:50,  2.79it/s, loss=3.8742, acc=0.402, ppl=48.1, lr=9.22e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 1860/2000 [11:45<00:50,  2.79it/s, loss=3.8666, acc=0.414, ppl=47.8, lr=9.21e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▎| 1870/2000 [11:48<00:45,  2.83it/s, loss=3.8666, acc=0.414, ppl=47.8, lr=9.21e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▎| 1870/2000 [11:49<00:45,  2.83it/s, loss=3.9386, acc=0.397, ppl=51.3, lr=9.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 1880/2000 [11:52<00:43,  2.79it/s, loss=3.9386, acc=0.397, ppl=51.3, lr=9.20e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 1880/2000 [11:53<00:43,  2.79it/s, loss=3.6901, acc=0.427, ppl=40.1, lr=9.18e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 1890/2000 [11:56<00:38,  2.84it/s, loss=3.6901, acc=0.427, ppl=40.1, lr=9.18e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 1890/2000 [11:56<00:38,  2.84it/s, loss=3.7664, acc=0.401, ppl=43.2, lr=9.18e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▌| 1900/2000 [11:59<00:35,  2.79it/s, loss=3.7664, acc=0.401, ppl=43.2, lr=9.18e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▌| 1900/2000 [12:00<00:35,  2.79it/s, loss=3.8173, acc=0.411, ppl=45.5, lr=9.16e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 1910/2000 [12:03<00:31,  2.84it/s, loss=3.8173, acc=0.411, ppl=45.5, lr=9.16e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 1910/2000 [12:03<00:31,  2.84it/s, loss=3.8423, acc=0.417, ppl=46.6, lr=9.15e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 1920/2000 [12:06<00:28,  2.79it/s, loss=3.8423, acc=0.417, ppl=46.6, lr=9.15e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 1920/2000 [12:07<00:28,  2.79it/s, loss=3.6431, acc=0.436, ppl=38.2, lr=9.14e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▋| 1930/2000 [12:10<00:24,  2.84it/s, loss=3.6431, acc=0.436, ppl=38.2, lr=9.14e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▋| 1930/2000 [12:10<00:24,  2.84it/s, loss=3.6187, acc=0.427, ppl=37.3, lr=9.13e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 1940/2000 [12:13<00:21,  2.79it/s, loss=3.6187, acc=0.427, ppl=37.3, lr=9.13e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 1940/2000 [12:14<00:21,  2.79it/s, loss=3.7013, acc=0.440, ppl=40.5, lr=9.12e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 1950/2000 [12:17<00:17,  2.84it/s, loss=3.7013, acc=0.440, ppl=40.5, lr=9.12e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 1950/2000 [12:17<00:17,  2.84it/s, loss=3.7290, acc=0.419, ppl=41.6, lr=9.11e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 1960/2000 [12:21<00:14,  2.79it/s, loss=3.7290, acc=0.419, ppl=41.6, lr=9.11e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 1960/2000 [12:21<00:14,  2.79it/s, loss=3.5732, acc=0.435, ppl=35.6, lr=9.10e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 1970/2000 [12:24<00:10,  2.84it/s, loss=3.5732, acc=0.435, ppl=35.6, lr=9.10e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 1970/2000 [12:24<00:10,  2.84it/s, loss=3.7698, acc=0.416, ppl=43.4, lr=9.09e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  99%|█████████▉| 1980/2000 [12:28<00:07,  2.79it/s, loss=3.7698, acc=0.416, ppl=43.4, lr=9.09e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training:  99%|█████████▉| 1980/2000 [12:28<00:07,  2.79it/s, loss=3.7675, acc=0.427, ppl=43.3, lr=9.07e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|█████████▉| 1990/2000 [12:31<00:03,  2.84it/s, loss=3.7675, acc=0.427, ppl=43.3, lr=9.07e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|█████████▉| 1990/2000 [12:32<00:03,  2.84it/s, loss=3.6458, acc=0.425, ppl=38.3, lr=9.06e-03]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|██████████| 2000/2000 [12:35<00:00,  2.65it/s, loss=3.6458, acc=0.425, ppl=38.3, lr=9.06e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ⏱️ Training completed in 755.4 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  📊 Final - Loss: 3.1932, Acc: 0.5054, PPL: 24.37\n",
            "💾 Saved final model to final_model.pt\n",
            "\n",
            "🎉 TRAINING COMPLETED!\n",
            "⏱️ Total time: 12.8 minutes\n",
            "🏆 Final Results:\n",
            "   Validation Loss: 3.1932\n",
            "   Validation Accuracy: 0.5054\n",
            "   Validation Perplexity: 24.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **17. Model Loading**\n",
        "\n",
        "After training, we can load our saved model."
      ],
      "metadata": {
        "id": "Or310BqX52_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_trained_model(model_path: str = \"final_model.pt\"):\n",
        "    \"\"\"\n",
        "    Load a trained MinimalLLM model from a checkpoint.\n",
        "\n",
        "    This function handles:\n",
        "        - Safe deserialization for PyTorch 2.6+.\n",
        "        - Loading model configuration and weights.\n",
        "        - Moving the model to GPU if available.\n",
        "        - Setting the model to evaluation mode.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the checkpoint file.\n",
        "\n",
        "    Returns:\n",
        "        model (nn.Module): Loaded MinimalLLM model ready for inference.\n",
        "        config (ModelConfig): Model configuration used for training.\n",
        "    \"\"\"\n",
        "    print(f\"🔄 Loading model from {model_path}\")\n",
        "\n",
        "    # Ensure ModelConfig is available for safe deserialization (PyTorch 2.6+)\n",
        "    from torch.serialization import add_safe_globals\n",
        "    add_safe_globals([ModelConfig])\n",
        "\n",
        "    # Load checkpoint\n",
        "    try:\n",
        "        # Standard loading\n",
        "        checkpoint = torch.load(model_path, map_location='cpu')\n",
        "        config = checkpoint['config']\n",
        "    except Exception as e:\n",
        "        # Fallback for potential PyTorch serialization differences\n",
        "        print(f\"⚠️ Warning: failed standard load. Retrying with weights_only=False...\")\n",
        "        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
        "        config = checkpoint['config']\n",
        "\n",
        "    # Initialize model with loaded config\n",
        "    model = MinimalLLM(config)\n",
        "\n",
        "    # Load weights into model\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Move model to available device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Set model to evaluation mode (disables dropout, etc.)\n",
        "    model.eval()\n",
        "\n",
        "    # Print summary\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"✅ Model loaded successfully\")\n",
        "    print(f\"   Parameters: {total_params:,}\")\n",
        "    print(f\"   Device: {device}\")\n",
        "\n",
        "    return model, config"
      ],
      "metadata": {
        "id": "KHvAPqJ24igR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **18a. Model Inference - Text Generation**"
      ],
      "metadata": {
        "id": "2idhGuNJ63c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(\n",
        "    model: nn.Module,\n",
        "    tokenizer,\n",
        "    prompt: str,\n",
        "    max_length: int = 100,\n",
        "    temperature: float = 0.8,\n",
        "    top_k: int = 50,\n",
        "    top_p: float = 0.9\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate text autoregressively using a trained language model.\n",
        "\n",
        "    The model predicts one token at a time, conditioned on the prompt and previously\n",
        "    generated tokens. Sampling is controlled by `temperature`, `top_k`, and `top_p`\n",
        "    (nucleus sampling) to balance creativity and coherence.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Trained language model.\n",
        "        tokenizer: Tokenizer for encoding and decoding text.\n",
        "        prompt (str): Initial text prompt to condition generation.\n",
        "        max_length (int): Maximum number of new tokens to generate.\n",
        "        temperature (float): Scales output logits — higher = more random sampling.\n",
        "        top_k (int): Keeps only the top-k most probable tokens.\n",
        "        top_p (float): Keeps the smallest set of tokens whose cumulative probability ≥ top_p.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text (prompt + model-generated continuation).\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure model is in inference mode (disables dropout, etc.)\n",
        "    model.eval()\n",
        "\n",
        "    # Detect which device (CPU/GPU) the model is on\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Convert prompt into token IDs and move to model's device\n",
        "    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt').to(device)\n",
        "\n",
        "    # This tensor will grow as we generate more tokens\n",
        "    generated_ids = input_ids.clone()\n",
        "\n",
        "    with torch.no_grad():  # Disable gradients for faster inference\n",
        "        for _ in range(max_length):\n",
        "            # 1. Forward pass: get logits for the current sequence\n",
        "            logits = model(generated_ids)\n",
        "\n",
        "            # 2. Extract the logits for the last token (the next-token prediction)\n",
        "            next_token_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "            # 3. Apply Top-K filtering (keep only the top K most likely tokens)\n",
        "            if top_k > 0:\n",
        "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
        "                filtered_logits = torch.full_like(next_token_logits, float('-inf'))\n",
        "                filtered_logits[top_k_indices] = top_k_logits\n",
        "                next_token_logits = filtered_logits\n",
        "\n",
        "            # 4. Apply Top-P (nucleus) filtering — keep tokens that make up top_p cumulative probability\n",
        "            if top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                # Mask out tokens beyond the top_p threshold\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
        "                sorted_indices_to_remove[0] = 0  # Always keep the highest probability token\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # 5. Sample next token based on filtered probabilities\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # 6. Append new token to the generated sequence\n",
        "            next_token = next_token.unsqueeze(0)  # Add batch dimension\n",
        "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
        "\n",
        "            # 7. Stop if the model predicts the end-of-sequence token\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # 8. Decode all generated tokens back into text\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "ralFRP7I62i7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_inference(model_path: str = \"final_model.pt\"):\n",
        "    \"\"\"\n",
        "    Run a quick text generation demo to showcase the model's capabilities.\n",
        "\n",
        "    This function automatically loads the trained model and tokenizer,\n",
        "    then generates short text samples from a list of example prompts.\n",
        "    It's a fast, hands-free way to verify that training and inference work correctly.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the saved model checkpoint file (default: \"final_model.pt\")\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"🎭 Running inference demo — showcasing model creativity and coherence\")\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # 1. Load the trained model and its configuration\n",
        "    # ------------------------------------------------------\n",
        "    model, config = load_trained_model(model_path)\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # 2. Load tokenizer (same as training tokenizer)\n",
        "    # ------------------------------------------------------\n",
        "    from transformers import AutoTokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
        "\n",
        "    # Ensure the tokenizer has a valid padding token\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # 3. Define demo prompts — diverse topics to test creativity\n",
        "    # ------------------------------------------------------\n",
        "    demo_prompts = [\n",
        "        \"A mysterious signal was detected from deep space\",\n",
        "        \"The future of human and AI collaboration is\",\n",
        "        \"Every morning, the robot barista greeted its customers by saying\",\n",
        "        \"In the ruins of an ancient digital city, explorers found\",\n",
        "        \"When the sun finally rose over the last surviving colony,\",\n",
        "        \"The secret ingredient to lifelong curiosity is\",\n",
        "        \"By 2099, memories became a form of currency\",\n",
        "        \"The scientist stared at the hologram and realized\",\n",
        "        \"In the middle of the storm, the android whispered\",\n",
        "        \"The code began to write itself when\"\n",
        "    ]\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # 4. Loop through each demo prompt and generate text\n",
        "    # ------------------------------------------------------\n",
        "    for i, prompt in enumerate(demo_prompts, 1):\n",
        "        print(f\"\\n🧠 Demo {i}: '{prompt}'\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Generate text for each prompt\n",
        "        generated_text = generate_text(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            prompt=prompt,\n",
        "            max_length=120,     # Slightly longer outputs for richness\n",
        "            temperature=0.7,    # Moderate creativity\n",
        "            top_k=40,           # Focus sampling on top 40 likely tokens\n",
        "            top_p=0.85          # Nucleus sampling for coherence and diversity\n",
        "        )\n",
        "\n",
        "        # Display generated result\n",
        "        print(f\"📝 {generated_text}\\n\")\n",
        "\n",
        "    print(\"✅ Demo complete — model generation looks healthy and coherent!\")\n"
      ],
      "metadata": {
        "id": "RmQCSqB016zN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check if we have a trained model\n",
        "    import os\n",
        "\n",
        "    if os.path.exists(\"final_model.pt\"):\n",
        "        print(\"🎉 Found trained model! Running demo...\")\n",
        "        demo_inference(\"final_model.pt\")\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️ No trained model found. Please run the training cells first.\")\n",
        "        print(\"💡 Look for 'final_model.pt' or 'best_model.pt' in your directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuSqZ_NS2UtU",
        "outputId": "a052d740-1747-4cb8-99a6-125d9c919bf6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉 Found trained model! Running demo...\n",
            "🎭 Running inference demo — showcasing model creativity and coherence\n",
            "🔄 Loading model from final_model.pt\n",
            "✅ Model loaded successfully\n",
            "   Parameters: 32,150,976\n",
            "   Device: cuda\n",
            "\n",
            "🧠 Demo 1: 'A mysterious signal was detected from deep space'\n",
            "------------------------------------------------------------\n",
            "📝 A mysterious signal was detected from deep space and create something called the 't'' comes the \"The Dottie.\"\n",
            "\n",
            "And so, they learned about this, and even more about their experiences. Brandon and its impact on how they found themselves.\n",
            "\n",
            "Emily took a small town's work. She looked like how to make his friends and even more about them. With a special places, Mr. Brandon smiled and asked Mr.\n",
            "\n",
            "\"I want my love.\"\n",
            "\n",
            "\"That sounds fascinating things, \"It means. But I see you have your favorite!\"\n",
            "\n",
            "Sally the fun and said,\n",
            "\n",
            "\n",
            "🧠 Demo 2: 'The future of human and AI collaboration is'\n",
            "------------------------------------------------------------\n",
            "📝 The future of human and AI collaboration is a significant role in shaping society. This section will delve into the concept of historical context of indigenous histories, specifically exploring historical context, exploring historical events, and societal values, and religious traditions while maintaining a deeper appreciation for global conflicts.\n",
            "\n",
            "II. The Intergenerational Gardening\n",
            "\n",
            "A. Historical Context of the context of modernizing Art\n",
            "\n",
            "In the importance of the United States, political parties must be seen as the late 190s, and early American nations. This section will explore how these historical events can lead to political parties significantly influenced American community.\n",
            "\n",
            "II. Understanding the\n",
            "\n",
            "\n",
            "🧠 Demo 3: 'Every morning, the robot barista greeted its customers by saying'\n",
            "------------------------------------------------------------\n",
            "📝 Every morning, the robot barista greeted its customers by saying \"It's because we know what we mean by \"HH!\"\n",
            "\n",
            "\"How does the key?\"\n",
            "\n",
            "Tama nodded her family, \"But wait, but don't have it mean it mean it mean to be much.\"\n",
            "\n",
            "She continued to be more. \"That sounds incredible places.\" He explained, but they noticed how many of the time for a good is all one of the other of the different places. They might need to learn more about different types of the rules before.\n",
            "\n",
            "And so, Mr. \"Well, it important to be able to learn about certain\n",
            "\n",
            "\n",
            "🧠 Demo 4: 'In the ruins of an ancient digital city, explorers found'\n",
            "------------------------------------------------------------\n",
            "📝 In the ruins of an ancient digital city, explorers found themselves!\n",
            "\n",
            "First, let's talk about what we mean by \"the time.\" It means that every time, it's talk about their own way of food so important to do with others because it's actually quite challenging to change their surroundings. Let's dive into this fascinating world!\n",
            "\n",
            "Imagine you have a new friends who lived during the world where people and friends who gets sick and family. This is exactly what happened during the same time. That's why learning how they're thinking about how we call a world we call water – to help us understand their own way of life, and\n",
            "\n",
            "\n",
            "🧠 Demo 5: 'When the sun finally rose over the last surviving colony,'\n",
            "------------------------------------------------------------\n",
            "📝 When the sun finally rose over the last surviving colony, the collector of the oven and even a slightly to the soil.\n",
            "\n",
            "Now, let's talk about something called the same material. If someone has a certain times, the same same problem is called a single step in the same 0s, and it has its core level. This is known as the \"Tele scale\" comes from the same system.\n",
            "\n",
            "Imagine being able to take a big store different places where people. That’s where something called \"the kind of like like like like when you come into play. These two friends can become more.\n",
            "Now, some people talk\n",
            "\n",
            "\n",
            "🧠 Demo 6: 'The secret ingredient to lifelong curiosity is'\n",
            "------------------------------------------------------------\n",
            "📝 The secret ingredient to lifelong curiosity is important details of learning and how they help keep our own bodies get better. Let's dive into this fascinating world together!\n",
            "\n",
            "Imagine being able to help you learn about the same time when you want to start by understanding the same day. Your friend, you feel like a friend, or something called an AGI brain is a tree, like a person, or \"Do-free shopping. That's where something called a computer-to-Futurism is like a type of something called a computer-made law! They use something called a type of a specific type of it all about.\n",
            "\n",
            "\n",
            "🧠 Demo 7: 'By 2099, memories became a form of currency'\n",
            "------------------------------------------------------------\n",
            "📝 By 2099, memories became a form of currency and other natural world. This tutorial will delve into various aspects of the complex network of the context of the lens of historical context of the realm of the context of juvenile fiction.\n",
            "\n",
            "To begin, let us define what a concept of \"The \"The Mahanta\" refers to the \"The Mahanta\" refers to the idea of \"The Mahanta\" refers to the collection of personal information as a type of self and \"The Mahanta\" works that allows individuals to the experiences, such as a \"The Mahanta\" plays in literature, characters, and opportunities, a form of personal experiences\n",
            "\n",
            "\n",
            "🧠 Demo 8: 'The scientist stared at the hologram and realized'\n",
            "------------------------------------------------------------\n",
            "📝 The scientist stared at the hologram and realized that the importance of managing potential benefits and promote growth.\n",
            "\n",
            "Now that we have it's where our own unique needs to understand its potential threats. For instance, let's dive into understanding what constitutes a \"Well,\" which involves looking at the importance of the world. This means they work together to be used to do we call them.\n",
            "\n",
            "Now, imagine if someone else's not a nearby store a small town? Well, imagine if every day had the same day would be like a company had the same day after the same same same day. That's what happened during the same day,\n",
            "\n",
            "\n",
            "🧠 Demo 9: 'In the middle of the storm, the android whispered'\n",
            "------------------------------------------------------------\n",
            "📝 In the middle of the storm, the android whispered our story of the heart of the beginning of the original original original world. They were known as the \"The First Regiment of the First Regiment of the Second Amendment as the Second Regiment of 300s and the Second Regiment of the Second Amendment as the Second Regiment of the Second Amendment as the Second Regiment of the Second Regiment.\n",
            "\n",
            "However, the Second Regiment of the Second Regiment of the Second Regiment by the Second Regiment of the Second Regiment by others to the Second Regiment of the Second Regiment of the Second Regiment of the Second Regiment of the Second Regiment of the Second Regiment by the Second Regiment\n",
            "\n",
            "\n",
            "🧠 Demo 10: 'The code began to write itself when'\n",
            "------------------------------------------------------------\n",
            "📝 The code began to write itself when they might go on the same line. By doing this, we can better understanding these concepts in our lives and how we help us better decisions about our bodies get better.\n",
            "\n",
            "First, let's dive into our journey through time! Imagine a simple example with your favorite video game where you want to see how this is called \"That's exactly which are different places like when we need to find the same thing.\n",
            "\n",
            "Now, imagine a friend, like a big big big game where the same time!\" He was actually want to get to take a big big, they decided to go on the\n",
            "\n",
            "✅ Demo complete — model generation looks healthy and coherent!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **18b. Model Inference - Chat Interaction**"
      ],
      "metadata": {
        "id": "sD9Q4yf77IOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_inference(model_path: str = \"final_model.pt\"):\n",
        "    \"\"\"\n",
        "    Launch an interactive text generation session with a trained language model.\n",
        "\n",
        "    The user enters prompts, and the model responds with generated text.\n",
        "    This simulates an interactive chat or creative writing interface.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the saved model checkpoint file.\n",
        "\n",
        "    Example:\n",
        "        >>> interactive_inference(\"final_model.pt\")\n",
        "        🤖 Starting interactive inference session\n",
        "        Enter your prompt: Once upon a time...\n",
        "        📝 Generated text: Once upon a time, in a faraway land...\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"🤖 Starting interactive inference session\")\n",
        "    print(\"Type 'quit' or 'exit' to stop\\n\")\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # 1. Load the trained model and its configuration\n",
        "    # ------------------------------------------------------\n",
        "    model, config = load_trained_model(model_path)\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # 2. Load the same tokenizer used during training\n",
        "    # ------------------------------------------------------\n",
        "    from transformers import AutoTokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
        "\n",
        "    # Ensure pad token is defined (some tokenizers don’t include it)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # 3. Continuous prompt-response loop\n",
        "    # ------------------------------------------------------\n",
        "    while True:\n",
        "        try:\n",
        "            # Get input prompt from the user\n",
        "            prompt = input(\"\\n🗨️  Enter your prompt: \").strip()\n",
        "\n",
        "            # Exit conditions\n",
        "            if prompt.lower() in {\"quit\", \"exit\", \"q\"}:\n",
        "                print(\"\\n👋 Goodbye!\")\n",
        "                break\n",
        "            if not prompt:\n",
        "                continue  # Skip empty input lines\n",
        "\n",
        "            # Generate text\n",
        "            print(\"\\n🔄 Generating response...\\n\")\n",
        "            generated_text = generate_text(\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                prompt=prompt,\n",
        "                max_length=150,\n",
        "                temperature=0.8,\n",
        "                top_k=50,\n",
        "                top_p=0.9\n",
        "            )\n",
        "\n",
        "            # Display the model's output\n",
        "            print(\"📝 Generated text:\\n\")\n",
        "            print(generated_text)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n👋 Session interrupted by user.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during generation: {e}\")"
      ],
      "metadata": {
        "id": "Sr7H0cUk7I3t"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check if we have a trained model\n",
        "    import os\n",
        "\n",
        "    if os.path.exists(\"final_model.pt\"):\n",
        "        interactive_inference(\"final_model.pt\")\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️ No trained model found. Please run the training cells first.\")\n",
        "        print(\"💡 Look for 'final_model.pt' or 'best_model.pt' in your directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShlMrffy17Tf",
        "outputId": "524b9d2e-d860-450f-e9fc-e225f59e871f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Starting interactive inference session\n",
            "Type 'quit' or 'exit' to stop\n",
            "\n",
            "🔄 Loading model from final_model.pt\n",
            "✅ Model loaded successfully\n",
            "   Parameters: 32,150,976\n",
            "   Device: cuda\n",
            "\n",
            "🗨️  Enter your prompt: hi\n",
            "\n",
            "🔄 Generating response...\n",
            "\n",
            "📝 Generated text:\n",
            "\n",
            "hi, the heart of the beginning of the world.\n",
            "\n",
            "Section 3: What is Cultural Heritage\n",
            "\n",
            "Imagine trying to bring friends and family, and family. To achieve this, people who lived, known as the world, and stories of us. However, many people lived there were some people called \"The Mahanta\" as a country. Let's learn about what makes it on the story and how they believe in today.\n",
            "\n",
            "Section 4: What does 'A \"The Mahanta\"\n",
            "\n",
            "* **\n",
            "* **B:** Our group has a particular group of \"The Mahanta\" by the \"The Mahanta\" through the United States, a \"The Mahanta\" through the \"The Mahanta\" by various\n",
            "\n",
            "👋 Session interrupted by user.\n"
          ]
        }
      ]
    }
  ]
}